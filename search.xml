<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Heart failure detection by SVM-based machine learning model</title>
    <url>/2021/01/24/YuxuanWu-1716309-INT305-Report/</url>
    <content><![CDATA[<p>Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide.<br>Heart failure is a common event caused by CVDs.</p>
<p>Most cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.</p>
<p>People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.</p>
<h3 id="Project-description-overview"><a href="#Project-description-overview" class="headerlink" title="Project description (overview)"></a>Project description (overview)</h3><p>The input to our predictor is is a medical dataset which contains 12 features that can be used to predict mortality by heart failure.</p>
<ol>
<li>Data exploration<ul>
<li>Principle Components Analysis (PCA) to reduce the dimension of features to have a view of the input data distribution</li>
<li>Build a preliminary linear SVM model to incorporate all the features to see the model performance.</li>
</ul>
</li>
<li>Feature selection part.<ul>
<li>Chi-square test to check the correlation between each categorical feature and the target death event.</li>
<li>Heat map to return the features with high correlation coefficient with death events.</li>
<li>Visualized the each feature’s contribution significance in the SVM model</li>
<li>Compared the returned features and determined the final selected features</li>
</ul>
</li>
<li>Model comparison and hyperparameter tuning<ul>
<li>compare the performance in difference preprocessing methods MinMaxScalar, StandardScalar, RobustScalar</li>
<li>compare the performance in k-fold cross validation and leave-one-out methods</li>
<li>compare the kernel selected in Support Vector Machine (linear or rbf)</li>
<li>grid search to find the best performance model</li>
</ul>
</li>
<li>Selected model performance<ul>
<li>calculated the precision, recall, accuracy and f1-score</li>
<li>plot the ROC and PR-curve</li>
<li>plot the learning curve</li>
</ul>
</li>
</ol>
<hr>
<h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><h3 id="Explorative-data-analysis-EDA-approach"><a href="#Explorative-data-analysis-EDA-approach" class="headerlink" title="Explorative data analysis (EDA) approach"></a>Explorative data analysis (EDA) approach</h3><ol>
<li><a class="link"   href="https://www.kaggle.com/nayansakhiya/heart-fail-analysis-and-quick-prediction" >heart-fail-analysis-and-quick-prediction<i class="fas fa-external-link-alt"></i></a></li>
</ol>
<p><strong>Strength</strong>: Detailed explorative and associative data analysis with great data visualization: each factor is visualized by different types of figures <br><br><strong>Weakness</strong>: Prediction model are quite rudimentary, the author did not select the features and tune the models’ hyperparameters.<br><br><strong>Similarity</strong>: I learned and applied the plotly.express API to create fancy and concise figures for easy comparison; I furthered his rudimentary model by optimization</p>
<h3 id="Predictive-data-analysis-PDA-approach"><a href="#Predictive-data-analysis-PDA-approach" class="headerlink" title="Predictive data analysis (PDA) approach"></a>Predictive data analysis (PDA) approach</h3><ol>
<li><a class="link"   href="https://www.kaggle.com/rude009/heart-failure-model-prediction-comparisons-95" >heart-failure-model-prediction-comparisons-95<i class="fas fa-external-link-alt"></i></a></li>
</ol>
<p><strong>Strength</strong>: The author compares six prediction models with feature selection. The Extra Gradient Booster Classifier could achieve the accuracy up to 95.0% <br><br><strong>Weakness</strong>: The author consider the “time” column as the useful features. <br><br><strong>Similarity</strong>: I don’t think “time” colume should be included since “time” column stands for Follow-up period (days), which means itself could not contribute the diseases itself. Therefore, I consider this feature as uselessness in our prediction model</p>
<ol start="2">
<li><a class="link"   href="https://www.kaggle.com/ksvmuralidhar/heart-failure-prediction-auc-0-98" >heart-failure-prediction-auc-0-98<i class="fas fa-external-link-alt"></i></a></li>
</ol>
<p><strong>Strength</strong>: The author uses a new method: Chi-square test to find the correlation between single categorical feature with target death_event <br><br><strong>Weakness</strong>: The visualization part does not as fancy as previous work <br><br><strong>Similarity</strong>: I learned and used the Chi-square test to conduct the correlation test between single categorical data with the categorical death_event; but i didn’t agree the author’s method in using box plot comparison between numerical data with categorical data, I used heat map instead.</p>
<hr>
<h2 id="Data-download"><a href="#Data-download" class="headerlink" title="Data download"></a>Data download</h2><p>You could simply download the data from my own Github repository: <a class="link"   href="https://media.githubusercontent.com/media/yuxuanwu17/kaggle/main/heart_failure_clinical_records_dataset.csv" >https://media.githubusercontent.com/media/yuxuanwu17/kaggle/main/heart_failure_clinical_records_dataset.csv<i class="fas fa-external-link-alt"></i></a></p>
<p>You could also download the data from kaggle：<br><a class="link"   href="https://www.kaggle.com/andrewmvd/heart-failure-clinical-data" >https://www.kaggle.com/andrewmvd/heart-failure-clinical-data<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="Libraries-used-in-this-project"><a href="#Libraries-used-in-this-project" class="headerlink" title="Libraries used in this project"></a>Libraries used in this project</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score, LeaveOneOut,GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler, StandardScaler, RobustScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve, classification_report</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> chi2_contingency</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sn</span><br><span class="line"><span class="keyword">import</span> plotly.express <span class="keyword">as</span> px</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Problem-formulation"><a href="#Problem-formulation" class="headerlink" title="Problem formulation"></a>Problem formulation</h2><h3 id="Return-the-head-of-dataset-a-overview-of-inside-components"><a href="#Return-the-head-of-dataset-a-overview-of-inside-components" class="headerlink" title="Return the head of dataset (a overview of inside components)"></a>Return the head of dataset (a overview of inside components)</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># df = pd.read_csv(&quot;/home/yuxuan/kaggle/heart_failure_clinical_records_dataset.csv&quot;)</span></span><br><span class="line">df = pd.read_csv(<span class="string">&quot;/Users/yuxuan/Desktop/kaggle/heart_failure_clinical_records_dataset.csv&quot;</span>)</span><br><span class="line"><span class="comment"># df = pd.read_csv(&quot;../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv&quot;)</span></span><br><span class="line">heart_data = df.copy()</span><br><span class="line">heart_data.head()</span><br></pre></td></tr></table></figure>



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>anaemia</th>
      <th>creatinine_phosphokinase</th>
      <th>diabetes</th>
      <th>ejection_fraction</th>
      <th>high_blood_pressure</th>
      <th>platelets</th>
      <th>serum_creatinine</th>
      <th>serum_sodium</th>
      <th>sex</th>
      <th>smoking</th>
      <th>time</th>
      <th>DEATH_EVENT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>75.0</td>
      <td>0</td>
      <td>582</td>
      <td>0</td>
      <td>20</td>
      <td>1</td>
      <td>265000.00</td>
      <td>1.9</td>
      <td>130</td>
      <td>1</td>
      <td>0</td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>55.0</td>
      <td>0</td>
      <td>7861</td>
      <td>0</td>
      <td>38</td>
      <td>0</td>
      <td>263358.03</td>
      <td>1.1</td>
      <td>136</td>
      <td>1</td>
      <td>0</td>
      <td>6</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>65.0</td>
      <td>0</td>
      <td>146</td>
      <td>0</td>
      <td>20</td>
      <td>0</td>
      <td>162000.00</td>
      <td>1.3</td>
      <td>129</td>
      <td>1</td>
      <td>1</td>
      <td>7</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>50.0</td>
      <td>1</td>
      <td>111</td>
      <td>0</td>
      <td>20</td>
      <td>0</td>
      <td>210000.00</td>
      <td>1.9</td>
      <td>137</td>
      <td>1</td>
      <td>0</td>
      <td>7</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>65.0</td>
      <td>1</td>
      <td>160</td>
      <td>1</td>
      <td>20</td>
      <td>0</td>
      <td>327000.00</td>
      <td>2.7</td>
      <td>116</td>
      <td>0</td>
      <td>0</td>
      <td>8</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<h3 id="Print-the-size-of-the-dataset"><a href="#Print-the-size-of-the-dataset" class="headerlink" title="Print the size of the dataset"></a>Print the size of the dataset</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(heart_data.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(299, 13)</code></pre>
<h3 id="Check-the-ratio-of-the-NaNs-for-each-column"><a href="#Check-the-ratio-of-the-NaNs-for-each-column" class="headerlink" title="Check the ratio of the NaNs for each column"></a>Check the ratio of the NaNs for each column</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> heart_data.columns:</span><br><span class="line">    print(col, <span class="built_in">str</span>(<span class="built_in">round</span>(<span class="number">100</span>* heart_data[col].isnull().<span class="built_in">sum</span>() / <span class="built_in">len</span>(heart_data), <span class="number">2</span>)) + <span class="string">&#x27;%&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>age 0.0%
anaemia 0.0%
creatinine_phosphokinase 0.0%
diabetes 0.0%
ejection_fraction 0.0%
high_blood_pressure 0.0%
platelets 0.0%
serum_creatinine 0.0%
serum_sodium 0.0%
sex 0.0%
smoking 0.0%
time 0.0%
DEATH_EVENT 0.0%</code></pre>
<hr>
<h3 id="Dataset-description"><a href="#Dataset-description" class="headerlink" title="Dataset description"></a>Dataset description</h3><p>There are 13 dimensions and 299 samples. All the columns are devoid of NaNs. We need make some rules before the data processing。</p>
<p>Target features (binary classification): DEATH_EVENT</p>
<p><strong>Categorical data</strong></p>
<ul>
<li>Sex - Gender of patient Male = 1, Female =0</li>
<li>Diabetes - 0 = No, 1 = Yes</li>
<li>Anaemia - 0 = No, 1 = Yes</li>
<li>High_blood_pressure - 0 = No, 1 = Yes</li>
<li>Smoking - 0 = No, 1 = Yes</li>
<li>DEATH_EVENT - 0 = No, 1 = Yes</li>
</ul>
<p><strong>Numerical data</strong></p>
<ul>
<li>Age - Age of patient</li>
<li>creatinine_phosphokinase - Level of the CPK enzyme in the blood (mcg/L)</li>
<li>ejection_fraction - Percentage of blood leaving the heart at each contraction (percentage)</li>
<li>platelets - Platelets in the blood (kiloplatelets/mL)</li>
<li>serum_creatinine - Level of serum creatinine in the blood (mg/dL)</li>
<li>serum_sodium - Level of serum sodium in the blood (mEq/L)</li>
<li>time - Follow-up period (days)</li>
</ul>
<h3 id="Citation-or-Reference"><a href="#Citation-or-Reference" class="headerlink" title="Citation or Reference"></a>Citation or Reference</h3><p>Dataset from Davide Chicco, Giuseppe Jurman: Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Medical Informatics and Decision Making 20, 16 (2020)</p>
<p>The dataset downloaded from Kaggle <a class="link"   href="https://www.kaggle.com/andrewmvd/heart-failure-clinical-data" >https://www.kaggle.com/andrewmvd/heart-failure-clinical-data<i class="fas fa-external-link-alt"></i></a></p>
<hr>
<h3 id="Principle-components-analysis"><a href="#Principle-components-analysis" class="headerlink" title="Principle components analysis"></a>Principle components analysis</h3><p>Reduce the dimensions and return the sample distribution</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">X = heart_data.iloc[:,<span class="number">0</span>:<span class="number">11</span>]</span><br><span class="line">X = StandardScaler().fit_transform(X)</span><br><span class="line">y = heart_data[<span class="string">&#x27;DEATH_EVENT&#x27;</span>]</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pca.fit(X)</span><br><span class="line">X_pca = pca.transform(X)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">plt.scatter(X_pca[y==<span class="number">0</span>,<span class="number">0</span>],X_pca[y==<span class="number">0</span>,<span class="number">1</span>],label=<span class="string">&quot;Alive&quot;</span>,c=<span class="string">&quot;red&quot;</span>)</span><br><span class="line">plt.scatter(X_pca[y==<span class="number">1</span>,<span class="number">0</span>],X_pca[y==<span class="number">1</span>,<span class="number">1</span>],label=<span class="string">&quot;Death&quot;</span>,c=<span class="string">&quot;black&quot;</span>)</span><br><span class="line">plt.legend([<span class="string">&quot;Alive&quot;</span>,<span class="string">&quot;Death&quot;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="http://i2.tiimg.com/732634/6142260ea33218d3.png" alt="Markdown"></p>
<p>I would like to have a view of the sample distribution in the dataset. Therefore, I used the principle component analysis (PCA) to reduce the dimension of features into 2D for visualization. The figure suggested that the data are densed and not easy to seperate (either linear or non-linear model), indicating the existence of insignificant feature which could negatively influence the future prediction accuracy.</p>
<hr>
<h3 id="Correlation-analysis"><a href="#Correlation-analysis" class="headerlink" title="Correlation analysis"></a>Correlation analysis</h3><p>I would like to find the correlation between each feature, especially with the target variable: DEATH_EVENT.<br>In this case, I excluded the column “time” since the time tracked could not contribute to the heart failure itself.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sn</span><br><span class="line">heart_data = heart_data.drop([<span class="string">&#x27;time&#x27;</span>],axis=<span class="number">1</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">12</span>))</span><br><span class="line">sn.heatmap(heart_data.corr(),vmin=-<span class="number">1</span>,cmap=<span class="string">&#x27;coolwarm&#x27;</span>,annot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><img src="http://i1.fuimg.com/732634/d2e422a950079a81.png" alt="Markdown"></p>
<h3 id="Data-partition"><a href="#Data-partition" class="headerlink" title="Data partition"></a>Data partition</h3><ul>
<li>As I mentioned before, I will not take the feature “time” into consideration. Therefore, 11 features are included in the final model prediction.</li>
<li>I split the dataset into two categories. 80% for raining data and 20% for testing data.</li>
<li>I used the StandardScalar normalization method to preprocess the data</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X = heart_data.iloc[:,<span class="number">0</span>:<span class="number">11</span>]</span><br><span class="line">X = StandardScaler().fit_transform(X)</span><br><span class="line">y = heart_data[<span class="string">&#x27;DEATH_EVENT&#x27;</span>]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=<span class="number">0.2</span>,random_state=<span class="number">2</span>)</span><br><span class="line">print(<span class="string">&quot;The number of training sample is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_train.shape[<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">&quot;The number of testing sample is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(X_test.shape[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>The number of training sample is 239
The number of testing sample is 60</code></pre>
<h3 id="Feature-selection-feature-engineering"><a href="#Feature-selection-feature-engineering" class="headerlink" title="Feature selection (feature engineering)"></a>Feature selection (feature engineering)</h3><p><strong>Method 1: Chi-square test</strong></p>
<ul>
<li>Based on previous research, I could conclude that DEATH_EVENT is our target. Since I have six categorical data I would like to figure out whether these single categorical valuable has significant correlation with the DEATH_EVENT.<br></li>
<li>Crosstables/contingency tables are one of the best ways to see how categorical variables are distributed among each other.</li>
<li>The following test suggests that we failed to reject the $H_0$ problem, indicating that there is no direct relationship between the DEATH_EVENT</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> chi2_contingency</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">heart_data = df.copy()</span><br><span class="line">cat_features = [<span class="string">&quot;anaemia&quot;</span>,<span class="string">&quot;diabetes&quot;</span>,<span class="string">&quot;high_blood_pressure&quot;</span>,<span class="string">&quot;sex&quot;</span>,<span class="string">&quot;smoking&quot;</span>,<span class="string">&quot;DEATH_EVENT&quot;</span>]</span><br><span class="line">num_features = pd.Series(heart_data.columns)</span><br><span class="line">num_features = num_features[~num_features.isin(cat_features)]</span><br><span class="line">num_features</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> cat_features:</span><br><span class="line">    ct = pd.crosstab(columns=heart_data[i],index=heart_data[<span class="string">&quot;DEATH_EVENT&quot;</span>])</span><br><span class="line">    stat, p, dof, expected = chi2_contingency(ct)</span><br><span class="line">    print(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    print(<span class="string">&#x27;-&#x27;</span>*<span class="built_in">len</span>(<span class="string">&#x27;CROSSTAB BETWEEN &#123;&#125; &amp; DEATH_EVENT&#x27;</span>.<span class="built_in">format</span>(i.upper())))</span><br><span class="line">    print(<span class="string">&quot;CROSSTAB BETWEEN &#123;&#125; &amp; DEATH_EVENT&quot;</span>.<span class="built_in">format</span>(i.upper()))</span><br><span class="line">    print(<span class="string">&#x27;-&#x27;</span>*<span class="built_in">len</span>(<span class="string">&#x27;CROSSTAB BETWEEN &#123;&#125; &amp; DEATH_EVENT&#x27;</span>.<span class="built_in">format</span>(i.upper())))</span><br><span class="line">    print(ct)</span><br><span class="line">    print(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    print(<span class="string">&quot;H0: THERE IS NO RELATIONSHIP BETWEEN DEATH_EVENT &amp; &#123;&#125; \nH1: THERE IS RELATIONSHIP BETWEEN DEATH_EVENT &amp; &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i.upper(),i.upper()))</span><br><span class="line">    print(<span class="string">&quot;\nP-VALUE: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(np.<span class="built_in">round</span>(p,<span class="number">2</span>)))</span><br><span class="line">    print(<span class="string">&quot;REJECT H0&quot;</span> <span class="keyword">if</span> p&lt;<span class="number">0.05</span> <span class="keyword">else</span> <span class="string">&quot;FAILED TO REJECT H0&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>​<br>    ————————————–<br>    CROSSTAB BETWEEN ANAEMIA &amp; DEATH_EVENT<br>    ————————————–<br>    anaemia        0   1<br>    DEATH_EVENT<br>    0            120  83<br>    1             50  46</p>
<p>​<br>    H0: THERE IS NO RELATIONSHIP BETWEEN DEATH_EVENT &amp; ANAEMIA<br>    H1: THERE IS RELATIONSHIP BETWEEN DEATH_EVENT &amp; ANAEMIA</p>
<pre><code>P-VALUE: 0.31
FAILED TO REJECT H0</code></pre>
<p>​<br>    —————————————<br>    CROSSTAB BETWEEN DIABETES &amp; DEATH_EVENT<br>    —————————————<br>    diabetes       0   1<br>    DEATH_EVENT<br>    0            118  85<br>    1             56  40</p>
<p>​<br>    H0: THERE IS NO RELATIONSHIP BETWEEN DEATH_EVENT &amp; DIABETES<br>    H1: THERE IS RELATIONSHIP BETWEEN DEATH_EVENT &amp; DIABETES</p>
<pre><code>P-VALUE: 0.93
FAILED TO REJECT H0</code></pre>
<p>​<br>    ————————————————–<br>    CROSSTAB BETWEEN HIGH_BLOOD_PRESSURE &amp; DEATH_EVENT<br>    ————————————————–<br>    high_blood_pressure    0   1<br>    DEATH_EVENT<br>    0                    137  66<br>    1                     57  39</p>
<p>​<br>    H0: THERE IS NO RELATIONSHIP BETWEEN DEATH_EVENT &amp; HIGH_BLOOD_PRESSURE<br>    H1: THERE IS RELATIONSHIP BETWEEN DEATH_EVENT &amp; HIGH_BLOOD_PRESSURE</p>
<pre><code>P-VALUE: 0.21
FAILED TO REJECT H0</code></pre>
<p>​<br>    ———————————-<br>    CROSSTAB BETWEEN SEX &amp; DEATH_EVENT<br>    ———————————-<br>    sex           0    1<br>    DEATH_EVENT<br>    0            71  132<br>    1            34   62</p>
<p>​<br>    H0: THERE IS NO RELATIONSHIP BETWEEN DEATH_EVENT &amp; SEX<br>    H1: THERE IS RELATIONSHIP BETWEEN DEATH_EVENT &amp; SEX</p>
<pre><code>P-VALUE: 0.96
FAILED TO REJECT H0</code></pre>
<p>​<br>    ————————————–<br>    CROSSTAB BETWEEN SMOKING &amp; DEATH_EVENT<br>    ————————————–<br>    smoking        0   1<br>    DEATH_EVENT<br>    0            137  66<br>    1             66  30</p>
<p>​<br>    H0: THERE IS NO RELATIONSHIP BETWEEN DEATH_EVENT &amp; SMOKING<br>    H1: THERE IS RELATIONSHIP BETWEEN DEATH_EVENT &amp; SMOKING</p>
<pre><code>P-VALUE: 0.93
FAILED TO REJECT H0</code></pre>
<p>​<br>    ——————————————<br>    CROSSTAB BETWEEN DEATH_EVENT &amp; DEATH_EVENT<br>    ——————————————<br>    DEATH_EVENT    0   1<br>    DEATH_EVENT<br>    0            203   0<br>    1              0  96</p>
<p>​<br>    H0: THERE IS NO RELATIONSHIP BETWEEN DEATH_EVENT &amp; DEATH_EVENT<br>    H1: THERE IS RELATIONSHIP BETWEEN DEATH_EVENT &amp; DEATH_EVENT</p>
<pre><code>P-VALUE: 0.0
REJECT H0</code></pre>
<p><strong>Method 2: correlation analysis</strong></p>
<ul>
<li>Use correlation coefficient &gt; 0.1 with death event</li>
<li>This method is applicable for both categorical data and numerical data</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_corr = heart_data.corr()</span><br><span class="line">feature_corr[<span class="built_in">abs</span>(feature_corr[<span class="string">&#x27;DEATH_EVENT&#x27;</span>]) &gt; <span class="number">0.1</span>][<span class="string">&#x27;DEATH_EVENT&#x27;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>age                  0.253729
ejection_fraction   -0.268603
serum_creatinine     0.294278
serum_sodium        -0.195204
time                -0.526964
DEATH_EVENT          1.000000
Name: DEATH_EVENT, dtype: float64</code></pre>
<p><strong>Method 3: Visualized plots of feature importance in linear SVM</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_plot</span>(<span class="params">classifier, feature_names, top_features=<span class="number">5</span></span>):</span></span><br><span class="line">    coef = classifier.coef_.ravel()</span><br><span class="line">    top_positive_coefficients = np.argsort(coef)[-top_features:]</span><br><span class="line">    top_negative_coefficients = np.argsort(coef)[:top_features]</span><br><span class="line">    middle_coefficient = np.argsort(coef)[top_features]</span><br><span class="line">    top_coefficients = np.hstack([top_negative_coefficients, middle_coefficient, top_positive_coefficients])</span><br><span class="line">    plt.figure(figsize=(<span class="number">18</span>, <span class="number">7</span>))</span><br><span class="line">    colors = [<span class="string">&#x27;green&#x27;</span> <span class="keyword">if</span> c &lt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;blue&#x27;</span> <span class="keyword">for</span> c <span class="keyword">in</span> coef[top_coefficients]]</span><br><span class="line">    plt.bar(np.arange(<span class="number">2</span> * top_features+<span class="number">1</span>), coef[top_coefficients], color=colors)</span><br><span class="line">    feature_names = np.array(feature_names)</span><br><span class="line">    plt.xticks(np.arange(<span class="number">2</span> * top_features+<span class="number">1</span>), feature_names[top_coefficients], rotation=<span class="number">45</span>, ha=<span class="string">&#x27;right&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line">heart_data = df.copy()</span><br><span class="line">X = heart_data.iloc[:, <span class="number">0</span>:<span class="number">11</span>]</span><br><span class="line">X = StandardScaler().fit_transform(X)</span><br><span class="line">y = heart_data[<span class="string">&#x27;DEATH_EVENT&#x27;</span>]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.1</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(heart_data.drop([&#x27;DEATH_EVENT&#x27;, &#x27;time&#x27;], axis=1).columns.values)</span></span><br><span class="line"></span><br><span class="line">trainedsvm = LinearSVC().fit(X, y)</span><br><span class="line">feature_plot(trainedsvm, heart_data.drop([<span class="string">&#x27;DEATH_EVENT&#x27;</span>, <span class="string">&#x27;time&#x27;</span>], axis=<span class="number">1</span>).columns.values)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="http://i1.fuimg.com/732634/12c3c71fe7a77951.png" alt="Markdown"></p>
<ul>
<li>The ahead plot illustrates the importance of feature in SVM model. y axis could be considered as weights and the absolute value of weights could suggest the contribution to the final results.</li>
<li>The correlation analysis between factors and death event returned the coefficient &gt; 0.1 features</li>
<li>Both analysis returned three same features: serum_creatinine, age, ejection_fraction</li>
<li>we need to evaluate and compare the performance in serum_sodium and creatinine_phosphokinase</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">heart_data = df.copy()</span><br><span class="line">X = heart_data.iloc[:, <span class="number">0</span>:<span class="number">11</span>]</span><br><span class="line">y = heart_data[<span class="string">&#x27;DEATH_EVENT&#x27;</span>]</span><br><span class="line">options = [<span class="string">&#x27;serum_sodium&#x27;</span>,<span class="string">&#x27;creatinine_phosphokinase&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> options:</span><br><span class="line">    selected_feature = [<span class="string">&#x27;serum_creatinine&#x27;</span>,<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;ejection_fraction&#x27;</span>]</span><br><span class="line">    selected_feature.append(i)</span><br><span class="line">    X_processed = X[selected_feature]</span><br><span class="line">    X_processed = StandardScaler().fit_transform(X_processed)</span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=<span class="number">0.2</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">1</span>)</span><br><span class="line">    sv_clf = SVC(kernel=<span class="string">&quot;linear&quot;</span>,random_state=<span class="number">1</span>)</span><br><span class="line">    sv_clf.fit(X_train, y_train)</span><br><span class="line">    sv_clf_pred = sv_clf.predict(X_test)</span><br><span class="line">    sv_clf_acc = accuracy_score(y_test,sv_clf_pred)</span><br><span class="line">    sc_clf_acc_format = <span class="built_in">round</span>(sv_clf_acc*<span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">#     accuracy_list.append(round(sv_clf_acc,2))</span></span><br><span class="line">    print(<span class="string">&quot;Accuracy of linear SVM model with feature &#123;&#125; is : &#123;&#125;%&quot;</span>.<span class="built_in">format</span>(i, sc_clf_acc_format))</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy of linear SVM model with feature serum_sodium is : 81.67%
Accuracy of linear SVM model with feature creatinine_phosphokinase is : 83.33%</code></pre>
<p>Therefore, four features including serum_creatinine, age, ejection_fraction, creatinine_phosphokinase</p>
<hr>
<h3 id="Visualized-the-learning-curve-after-feature-selection"><a href="#Visualized-the-learning-curve-after-feature-selection" class="headerlink" title="Visualized the learning curve after feature selection"></a>Visualized the learning curve after feature selection</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curve</span>(<span class="params">estimator, title, X, y, axes=<span class="literal">None</span>, ylim=<span class="literal">None</span>, cv=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                        n_jobs=<span class="literal">None</span>, train_sizes=np.linspace(<span class="params"><span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span></span>)</span>):</span></span><br><span class="line">    <span class="keyword">if</span> axes <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        _, axes = plt.subplots(<span class="number">1</span>, <span class="number">3</span>, figsize=(<span class="number">20</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    axes[<span class="number">0</span>].set_title(title)</span><br><span class="line">    <span class="keyword">if</span> ylim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        axes[<span class="number">0</span>].set_ylim(*ylim)</span><br><span class="line">    axes[<span class="number">0</span>].set_xlabel(<span class="string">&quot;Training examples&quot;</span>)</span><br><span class="line">    axes[<span class="number">0</span>].set_ylabel(<span class="string">&quot;Score&quot;</span>)</span><br><span class="line"></span><br><span class="line">    train_sizes, train_scores, test_scores, fit_times, _ = \</span><br><span class="line">        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,</span><br><span class="line">                       train_sizes=train_sizes,</span><br><span class="line">                       return_times=<span class="literal">True</span>)</span><br><span class="line">    train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    fit_times_mean = np.mean(fit_times, axis=<span class="number">1</span>)</span><br><span class="line">    fit_times_std = np.std(fit_times, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot learning curve</span></span><br><span class="line">    axes[<span class="number">0</span>].grid()</span><br><span class="line">    axes[<span class="number">0</span>].fill_between(train_sizes, train_scores_mean - train_scores_std,</span><br><span class="line">                         train_scores_mean + train_scores_std, alpha=<span class="number">0.1</span>,</span><br><span class="line">                         color=<span class="string">&quot;r&quot;</span>)</span><br><span class="line">    axes[<span class="number">0</span>].fill_between(train_sizes, test_scores_mean - test_scores_std,</span><br><span class="line">                         test_scores_mean + test_scores_std, alpha=<span class="number">0.1</span>,</span><br><span class="line">                         color=<span class="string">&quot;g&quot;</span>)</span><br><span class="line">    axes[<span class="number">0</span>].plot(train_sizes, train_scores_mean, <span class="string">&#x27;o-&#x27;</span>, color=<span class="string">&quot;r&quot;</span>,</span><br><span class="line">                 label=<span class="string">&quot;Training score&quot;</span>)</span><br><span class="line">    axes[<span class="number">0</span>].plot(train_sizes, test_scores_mean, <span class="string">&#x27;o-&#x27;</span>, color=<span class="string">&quot;g&quot;</span>,</span><br><span class="line">                 label=<span class="string">&quot;Cross-validation score&quot;</span>)</span><br><span class="line">    axes[<span class="number">0</span>].legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot n_samples vs fit_times</span></span><br><span class="line">    axes[<span class="number">1</span>].grid()</span><br><span class="line">    axes[<span class="number">1</span>].plot(train_sizes, fit_times_mean, <span class="string">&#x27;o-&#x27;</span>)</span><br><span class="line">    axes[<span class="number">1</span>].fill_between(train_sizes, fit_times_mean - fit_times_std,</span><br><span class="line">                         fit_times_mean + fit_times_std, alpha=<span class="number">0.1</span>)</span><br><span class="line">    axes[<span class="number">1</span>].set_xlabel(<span class="string">&quot;Training examples&quot;</span>)</span><br><span class="line">    axes[<span class="number">1</span>].set_ylabel(<span class="string">&quot;fit_times&quot;</span>)</span><br><span class="line">    axes[<span class="number">1</span>].set_title(<span class="string">&quot;Scalability of the model&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot fit_time vs score</span></span><br><span class="line">    axes[<span class="number">2</span>].grid()</span><br><span class="line">    axes[<span class="number">2</span>].plot(fit_times_mean, test_scores_mean, <span class="string">&#x27;o-&#x27;</span>)</span><br><span class="line">    axes[<span class="number">2</span>].fill_between(fit_times_mean, test_scores_mean - test_scores_std,</span><br><span class="line">                         test_scores_mean + test_scores_std, alpha=<span class="number">0.1</span>)</span><br><span class="line">    axes[<span class="number">2</span>].set_xlabel(<span class="string">&quot;fit_times&quot;</span>)</span><br><span class="line">    axes[<span class="number">2</span>].set_ylabel(<span class="string">&quot;Score&quot;</span>)</span><br><span class="line">    axes[<span class="number">2</span>].set_title(<span class="string">&quot;Performance of the model&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> plt</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig, axes = plt.subplots(<span class="number">3</span>, <span class="number">2</span>, figsize=(<span class="number">10</span>, <span class="number">15</span>))</span><br><span class="line"></span><br><span class="line">heart_data = df.copy()</span><br><span class="line"></span><br><span class="line">X = heart_data.iloc[:, <span class="number">0</span>:<span class="number">11</span>]</span><br><span class="line">y = heart_data[<span class="string">&#x27;DEATH_EVENT&#x27;</span>]</span><br><span class="line"></span><br><span class="line">selected_feature = [<span class="string">&#x27;serum_creatinine&#x27;</span>,<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;ejection_fraction&#x27;</span>,<span class="string">&#x27;creatinine_phosphokinase&#x27;</span>]</span><br><span class="line">X_processed = X[selected_feature]</span><br><span class="line">X_processed = StandardScaler().fit_transform(X_processed)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">title = <span class="string">&quot;Learning Curves (SVM, linear kernel)&quot;</span></span><br><span class="line"><span class="comment"># cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)</span></span><br><span class="line"></span><br><span class="line">estimator = SVC(kernel=<span class="string">&quot;linear&quot;</span>,random_state=<span class="number">1</span>)</span><br><span class="line">plot_learning_curve(estimator, title, X_processed, y, axes=axes[:, <span class="number">0</span>], ylim=(<span class="number">0.5</span>, <span class="number">1.01</span>),</span><br><span class="line">                     n_jobs=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">title = <span class="string">r&quot;Learning Curves (SVM, RBF kernel)&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)</span></span><br><span class="line">estimator = SVC(kernel=<span class="string">&quot;rbf&quot;</span>,random_state=<span class="number">1</span>)</span><br><span class="line">plot_learning_curve(estimator, title, X_processed, y, axes=axes[:, <span class="number">1</span>], ylim=(<span class="number">0.5</span>, <span class="number">1.01</span>),</span><br><span class="line">                     n_jobs=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="http://i1.fuimg.com/732634/79316f216c960b29.png" alt="Markdown"></p>
<ul>
<li><p>First column is the combination of learning curves, model scability, model performance in a SVM model with linear kernel. First row is the learning curve of linear SVM model: the training score is very high at the beginning and decreases and the cross-validation score is very low at the beginning and increases. The training score and the cross-validation score intertwines at about 220 training samples, and their difference after that are not significant</p>
</li>
<li><p>Second column is the combination of learning curves, model scability, model performance in a SVM model with linear kernel. The learning curve plots indicate that accuracy for both training score and cross validation score tend to be stable after 130 samples, which is similar in linear SVM model. The fit time for RBF kernel SVM is higher than linear kernel because of the complexity in calculation</p>
</li>
<li><p>RBF kernel has a relative higher performance, but the difference is not significant</p>
</li>
<li><p>The figure above doesn’t indicate either overfitting or underfitting problems</p>
</li>
</ul>
<hr>
<h3 id="Explorative-data-analysis-of-the-four-selected-features"><a href="#Explorative-data-analysis-of-the-four-selected-features" class="headerlink" title="Explorative data analysis of the four selected features:"></a>Explorative data analysis of the four selected features:</h3><ul>
<li>‘serum_creatinine’</li>
<li>‘age’</li>
<li>‘ejection_fraction’</li>
<li>‘creatinine_phosphokinase’</li>
</ul>
<h4 id="Specify-the-figure-size"><a href="#Specify-the-figure-size" class="headerlink" title="Specify the figure size"></a>Specify the figure size</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">HEIGHT = <span class="number">500</span></span><br><span class="line">WIDTH = <span class="number">700</span></span><br><span class="line">NBINS = <span class="number">50</span></span><br><span class="line">SCATTER_SIZE=<span class="number">700</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## Define the histogram</span></span><br><span class="line"><span class="keyword">import</span> plotly.express <span class="keyword">as</span> px</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_histogram</span>(<span class="params">dataframe, column, color, bins, marginal,title, width=WIDTH, height=HEIGHT</span>):</span></span><br><span class="line">    figure = px.histogram(</span><br><span class="line">        dataframe,</span><br><span class="line">        column,</span><br><span class="line">        color=color,</span><br><span class="line">        nbins=bins,</span><br><span class="line">        marginal= marginal,</span><br><span class="line">        title=title,</span><br><span class="line">        width=width,</span><br><span class="line">        height=height</span><br><span class="line">    )</span><br><span class="line">    figure.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_histogram(heart_data, <span class="string">&#x27;serum_creatinine&#x27;</span>, <span class="string">&#x27;DEATH_EVENT&#x27;</span>, NBINS, <span class="string">&quot;violin&quot;</span>,<span class="string">&#x27;Figure 1: Distribution of serum creatinine VS death event&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="http://i2.tiimg.com/732634/c3cf27edc28fd595.png" alt="Markdown"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_histogram(heart_data, <span class="string">&#x27;ejection_fraction&#x27;</span>, <span class="string">&#x27;DEATH_EVENT&#x27;</span>, NBINS, <span class="string">&quot;violin&quot;</span>,<span class="string">&#x27;Figure 2: Distribution of ejection fraction VS death event&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="http://i2.tiimg.com/732634/1643949fc06a2d10.png" alt="Markdown"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_histogram(heart_data, <span class="string">&#x27;creatinine_phosphokinase&#x27;</span>, <span class="string">&#x27;DEATH_EVENT&#x27;</span>, NBINS, <span class="string">&quot;violin&quot;</span>,<span class="string">&#x27;Figure 3: Distribution of creatinine_phosphokinase VS death event&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="http://i2.tiimg.com/732634/2d657b2005256d56.png" alt="Markdown"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> plotly.graph_objs <span class="keyword">as</span> go</span><br><span class="line">heart_data = df.copy()</span><br><span class="line">male = heart_data[heart_data[<span class="string">&quot;sex&quot;</span>]==<span class="number">1</span>]</span><br><span class="line">female = heart_data[heart_data[<span class="string">&quot;sex&quot;</span>]==<span class="number">0</span>]</span><br><span class="line">male_survival= male[male[<span class="string">&quot;DEATH_EVENT&quot;</span>]==<span class="number">0</span>]</span><br><span class="line">female_survival= female[female[<span class="string">&quot;DEATH_EVENT&quot;</span>]==<span class="number">0</span>]</span><br><span class="line"><span class="comment">## assign the labels</span></span><br><span class="line">labels = [<span class="string">&#x27;Male - Survived&#x27;</span>,<span class="string">&#x27;Male - Not Survived&#x27;</span>, <span class="string">&quot;Female -  Survived&quot;</span>, <span class="string">&quot;Female - Not Survived&quot;</span>]</span><br><span class="line"><span class="comment">## value is set according to the labels</span></span><br><span class="line">values = [<span class="built_in">len</span>(male[heart_data[<span class="string">&quot;DEATH_EVENT&quot;</span>]==<span class="number">0</span>]),<span class="built_in">len</span>(male[heart_data[<span class="string">&quot;DEATH_EVENT&quot;</span>]==<span class="number">1</span>]),</span><br><span class="line">         <span class="built_in">len</span>(female[heart_data[<span class="string">&quot;DEATH_EVENT&quot;</span>]==<span class="number">0</span>]),<span class="built_in">len</span>(female[heart_data[<span class="string">&quot;DEATH_EVENT&quot;</span>]==<span class="number">1</span>])]</span><br><span class="line">fig = go.Figure(data=[go.Pie(labels=labels,values=values,hole=<span class="number">.3</span>)])</span><br><span class="line">fig.update_layout(</span><br><span class="line">    title_text = <span class="string">&quot;Figure 4: Analysis on Survival - Gender factor&quot;</span></span><br><span class="line">)</span><br><span class="line">fig.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://i2.tiimg.com/732634/f3accef6dfd2ee38.png" alt="Markdown"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">## Define the violin plot function method</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">violin_boxplot</span>(<span class="params">dataframe, x, y,color,points,hover_data, box, width=WIDTH, height=HEIGHT</span>):</span></span><br><span class="line">    figure = px.violin(</span><br><span class="line">        dataframe,</span><br><span class="line">        x=x,</span><br><span class="line">        y=y,</span><br><span class="line">        color = color,</span><br><span class="line">        box = box,</span><br><span class="line">        hover_data=hover_data,</span><br><span class="line">        points=points,</span><br><span class="line">        width=width,</span><br><span class="line">        height=height</span><br><span class="line">    )</span><br><span class="line">    figure.update_layout(title_text=<span class="string">&quot;Figure 5: Analysis of both gender and age factors in survival rates&quot;</span>)</span><br><span class="line">    figure.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">violin_boxplot(heart_data,x = <span class="string">&quot;sex&quot;</span>,y=<span class="string">&quot;age&quot;</span>,color=<span class="string">&quot;DEATH_EVENT&quot;</span>,points=<span class="string">&quot;all&quot;</span>,box=<span class="literal">True</span>,hover_data=heart_data.columns)</span><br></pre></td></tr></table></figure>
<p><img src="http://i2.tiimg.com/732634/7261b8fdd617408a.png" alt="Markdown"></p>
<p>Figure 1 - Figure 5 is the visualization of each feature, there is no clear patterns or strong association between the death_event. Therefore, we need to further our research by conducting the model prediction process.</p>
<h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><h3 id="Python-library"><a href="#Python-library" class="headerlink" title="Python library"></a>Python library</h3><h1 id="Description-how-you-learned-the-predictor"><a href="#Description-how-you-learned-the-predictor" class="headerlink" title="Description: how you learned the predictor"></a>Description: how you learned the predictor</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br></pre></td></tr></table></figure>
<h3 id="PCA-to-visualize-the-sample-distribution"><a href="#PCA-to-visualize-the-sample-distribution" class="headerlink" title="PCA to visualize the sample distribution"></a>PCA to visualize the sample distribution</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pca.fit(X_processed)</span><br><span class="line">X_pca = pca.transform(X_processed)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">plt.scatter(X_pca[y==<span class="number">0</span>,<span class="number">0</span>],X_pca[y==<span class="number">0</span>,<span class="number">1</span>],label=<span class="string">&quot;Alive&quot;</span>,c=<span class="string">&quot;red&quot;</span>)</span><br><span class="line">plt.scatter(X_pca[y==<span class="number">1</span>,<span class="number">0</span>],X_pca[y==<span class="number">1</span>,<span class="number">1</span>],label=<span class="string">&quot;Death&quot;</span>,c=<span class="string">&quot;black&quot;</span>)</span><br><span class="line">plt.legend([<span class="string">&quot;Alive&quot;</span>,<span class="string">&quot;Death&quot;</span>])</span><br></pre></td></tr></table></figure>



<pre><code>&lt;matplotlib.legend.Legend at 0x7fdff0ee0d90&gt;</code></pre>
<p><img src="http://i1.fuimg.com/732634/2c89446d39ad4ab7.png" alt="Markdown"></p>
<p>In this case, the data distribution after feature selection are scattered, which could be beneficial for separation. Still, we could not determine whether linear kernel or RBF kernel is suitable for classification. I then would compare the performance between these two methods.</p>
<h3 id="Machine-learning-algorithms-with-description"><a href="#Machine-learning-algorithms-with-description" class="headerlink" title="Machine learning algorithms with description"></a>Machine learning algorithms with description</h3><h4 id="SVM-with-linear-kernel"><a href="#SVM-with-linear-kernel" class="headerlink" title="SVM with linear kernel"></a>SVM with linear kernel</h4><p>$$ K(x,y) = X^Ty=x\cdot y$$</p>
<p>Loss function: hinge loss / squared hinge loss</p>
<p>$$ Agreement: z = y_i(w \cdot x_i + \alpha) $$</p>
<p>Hinge loss</p>
<p>$$<br>L_h(z)= \begin{cases}<br>0 &amp; \text{if z$\geq$1}\<br>1-z &amp; \text{z&lt;1}<br>\end{cases}<br>$$</p>
<p>Squared hinge loss</p>
<p>$$<br>L_{hsqr}(z)= \begin{cases}<br>0 &amp; \text{if z$\geq$1}\<br>(1-z)^2 &amp; \text{z&lt;1}<br>\end{cases}<br>$$</p>
<p>Optimization objective formula for hinge loss:</p>
<p>$$<br>J(w,\alpha) = \frac{1}{n}\sum_{i=1}^nL_h(y_i(w\cdot x_i + \alpha))+\frac{\lambda}{2}(||w||)^2<br>$$</p>
<p>Description:</p>
<ul>
<li>Linear Kernel is used when the data is linearly separable dataset.</li>
<li>One of the goal is to minimize the previous objective formula for the hinge loss. $\lambda$ in this case stands for the regularization hyperparameter.</li>
<li>The strength of the regularization is inversely proportional to $\lambda$, it has to be strictly positive. The smaller regularization parameter means less tolerant to misclassification.</li>
<li>Require grid serach to return the suitable hyperparameter</li>
</ul>
<h4 id="SVM-with-RBF-kernel"><a href="#SVM-with-RBF-kernel" class="headerlink" title="SVM with RBF kernel"></a>SVM with RBF kernel</h4><p>$$ K(x,y) = e^{-\gamma||x-y||^2}, \gamma &gt;0 $$</p>
<p>Loss function: hinge loss / squared hinge loss</p>
<p>$$ Agreement: z = y_i(w \cdot \phi(x_i) + \alpha) $$</p>
<p>Hinge loss</p>
<p>$$<br>L_h(z)= \begin{cases}<br>0 &amp; \text{if z$\geq$1}\<br>1-z &amp; \text{z&lt;1}<br>\end{cases}<br>$$</p>
<p>Squared hinge loss</p>
<p>$$<br>L_{hsqr}(z)= \begin{cases}<br>0 &amp; \text{if z$\geq$1}\<br>(1-z)^2 &amp; \text{z&lt;1}<br>\end{cases}<br>$$</p>
<p>Optimization objective formula for hinge loss:</p>
<p>$$<br>J(w,\alpha) = \frac{1}{n}\sum_{i=1}^nL_h(y_i(w\cdot \phi(x_i) + \alpha))+\frac{\lambda}{2}(||w||)^2<br>$$</p>
<p>Description:</p>
<ul>
<li>SVM with RBF kernel was utilized to solve the linearly inseparable probelms</li>
<li>kernel trick was used in RBF kernel SVM to increase the computational efficiency</li>
<li>$\gamma$ parameter how far the influence of a single training example reaches</li>
<li>C parameter trades off correct classification of training examples against maximization of the decision function’s margin</li>
</ul>
<h4 id="Evaluation-method"><a href="#Evaluation-method" class="headerlink" title="Evaluation method"></a>Evaluation method</h4><p>Mean squared error (MSE)</p>
<p>$$<br>MSE = \frac{1}{n}\sum_{i=1}^n(Y_i- \hat{Y_i})^2<br>$$</p>
<p>where $Y_i$ is the label and $\hat{Y_i}$ is the predicted label by model</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">heart_data = df.copy()</span><br><span class="line"></span><br><span class="line">X = heart_data.iloc[:, <span class="number">0</span>:<span class="number">11</span>]</span><br><span class="line">y = heart_data[<span class="string">&#x27;DEATH_EVENT&#x27;</span>]</span><br><span class="line"></span><br><span class="line">selected_feature = [<span class="string">&#x27;serum_creatinine&#x27;</span>,<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;ejection_fraction&#x27;</span>,<span class="string">&#x27;creatinine_phosphokinase&#x27;</span>]</span><br><span class="line">X_processed = X[selected_feature]</span><br><span class="line">X_processed = StandardScaler().fit_transform(X_processed)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=<span class="number">0.2</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## linear svm</span></span><br><span class="line"><span class="comment"># sv_clf = LinearSVC(loss=&#x27;hinge&#x27;,random_state=1, C=1.0, penalty = &#x27;l2&#x27;)</span></span><br><span class="line">sv_clf = SVC(kernel=<span class="string">&quot;linear&quot;</span>,random_state=<span class="number">1</span>)</span><br><span class="line">sv_clf.fit(X_train, y_train)</span><br><span class="line">sv_clf_pred = sv_clf.predict(X_test)</span><br><span class="line">sv_clf_acc = accuracy_score(y_test,sv_clf_pred)</span><br><span class="line">sv_clf_mse = mean_squared_error(y_test,sv_clf_pred)</span><br><span class="line">print(<span class="string">&quot;Accuracy of linear SVM model is :&quot;</span>,<span class="string">&quot;&#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(<span class="number">100</span>*sv_clf_acc))</span><br><span class="line">print(<span class="string">&quot;Mean squared error of linear SVM model is :&quot;</span>,<span class="string">&quot;&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(sv_clf_mse))</span><br><span class="line"></span><br><span class="line"><span class="comment">## RBF kernel SVM</span></span><br><span class="line">sv_clf = SVC(kernel=<span class="string">&quot;rbf&quot;</span>,random_state=<span class="number">1</span>, C=<span class="number">1.0</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line">sv_clf.fit(X_train, y_train)</span><br><span class="line">sv_clf_pred = sv_clf.predict(X_test)</span><br><span class="line">sv_clf_acc = accuracy_score(y_test,sv_clf_pred)</span><br><span class="line">sv_clf_mse = mean_squared_error(y_test,sv_clf_pred)</span><br><span class="line">print(<span class="string">&quot;Accuracy of RBF SVM model is :&quot;</span>,<span class="string">&quot;&#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(<span class="number">100</span>*sv_clf_acc))</span><br><span class="line">print(<span class="string">&quot;Mean squared error of linear SVM model is :&quot;</span>,<span class="string">&quot;&#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(sv_clf_mse))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>Accuracy of linear SVM model is : 83.33%
Mean squared error of linear SVM model is : 0.17
Accuracy of RBF SVM model is : 86.67%
Mean squared error of linear SVM model is : 0.13</code></pre>
<h2 id="Experiments-and-results"><a href="#Experiments-and-results" class="headerlink" title="Experiments and results"></a>Experiments and results</h2><h3 id="Compare-the-efficacy-of-different-preprocessing-methods"><a href="#Compare-the-efficacy-of-different-preprocessing-methods" class="headerlink" title="Compare the efficacy of different preprocessing methods"></a>Compare the efficacy of different preprocessing methods</h3><ul>
<li>MinMaxScalar</li>
<li>StandardScalar</li>
<li>RobustScalar</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler, StandardScaler, RobustScaler</span><br><span class="line"></span><br><span class="line">heart_data = df.copy()</span><br><span class="line"></span><br><span class="line">X = heart_data.iloc[:, <span class="number">0</span>:<span class="number">11</span>]</span><br><span class="line">y = heart_data[<span class="string">&#x27;DEATH_EVENT&#x27;</span>]</span><br><span class="line"></span><br><span class="line">selected_feature = [<span class="string">&#x27;serum_creatinine&#x27;</span>,<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;ejection_fraction&#x27;</span>,<span class="string">&#x27;creatinine_phosphokinase&#x27;</span>]</span><br><span class="line">X_processed = X[selected_feature]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=<span class="number">0.2</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># MinMaxScalar</span></span><br><span class="line">pipe1 = Pipeline([(<span class="string">&quot;scalar&quot;</span>,MinMaxScaler()),(<span class="string">&quot;svm&quot;</span>,SVC(random_state=<span class="number">1</span>))])</span><br><span class="line">pipe1.fit(X_train,y_train)</span><br><span class="line">print(<span class="string">&quot;Test score for MinMaxScalar RBF kernel: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(pipe1.score(X_test,y_test)))</span><br><span class="line"></span><br><span class="line">pipe1_linear = Pipeline([(<span class="string">&quot;scalar&quot;</span>,MinMaxScaler()),(<span class="string">&quot;svm&quot;</span>,SVC(kernel=<span class="string">&quot;linear&quot;</span>,random_state=<span class="number">1</span>))])</span><br><span class="line">pipe1_linear.fit(X_train,y_train)</span><br><span class="line">print(<span class="string">&quot;Test score for MinMaxScalar linear kernel: &#123;:.3f&#125;\n&quot;</span>.<span class="built_in">format</span>(pipe1_linear.score(X_test,y_test)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># StandardScalar</span></span><br><span class="line"></span><br><span class="line">pipe2 = Pipeline([(<span class="string">&quot;scalar&quot;</span>,StandardScaler()),(<span class="string">&quot;svm&quot;</span>,SVC(random_state=<span class="number">1</span>))])</span><br><span class="line">pipe2.fit(X_train,y_train)</span><br><span class="line">print(<span class="string">&quot;Test score for StandardScalar in RBF kernel: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(pipe2.score(X_test,y_test)))</span><br><span class="line"></span><br><span class="line">pipe2_linear = Pipeline([(<span class="string">&quot;scalar&quot;</span>,StandardScaler()),(<span class="string">&quot;svm&quot;</span>,SVC(kernel=<span class="string">&quot;linear&quot;</span> ,random_state=<span class="number">1</span>))])</span><br><span class="line">pipe2_linear.fit(X_train,y_train)</span><br><span class="line">print(<span class="string">&quot;Test score for StandardScalar in linear kernel: &#123;:.3f&#125;\n&quot;</span>.<span class="built_in">format</span>(pipe2_linear.score(X_test,y_test)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># RobustScalar</span></span><br><span class="line"></span><br><span class="line">pipe3 = Pipeline([(<span class="string">&quot;scalar&quot;</span>,RobustScaler()),(<span class="string">&quot;svm&quot;</span>,SVC(random_state=<span class="number">1</span>))])</span><br><span class="line">pipe3.fit(X_train,y_train)</span><br><span class="line">print(<span class="string">&quot;Test score for RobustScalar in RBF kernel: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(pipe3.score(X_test,y_test)))</span><br><span class="line"></span><br><span class="line">pipe3_linear = Pipeline([(<span class="string">&quot;scalar&quot;</span>,RobustScaler()),(<span class="string">&quot;svm&quot;</span>,SVC(kernel = <span class="string">&quot;linear&quot;</span>,random_state=<span class="number">1</span>))])</span><br><span class="line">pipe3_linear.fit(X_train,y_train)</span><br><span class="line">print(<span class="string">&quot;Test score for RobustScalar in linear kernel: &#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(pipe3_linear.score(X_test,y_test)))</span><br></pre></td></tr></table></figure>
<pre><code>Test score for MinMaxScalar RBF kernel: 0.850
Test score for MinMaxScalar linear kernel: 0.767

Test score for StandardScalar in RBF kernel: 0.850
Test score for StandardScalar in linear kernel: 0.833

Test score for RobustScalar in RBF kernel: 0.800
Test score for RobustScalar in linear kernel: 0.817</code></pre>
<ul>
<li>There is no significant difference between different preprocessing method in RBF kernels, especially in MinMaxScalar and StandardScalar.</li>
<li>Overall RBF kernel outperforms than linear kernel.</li>
<li>In this case, I would insist on using StandardScalar in following procedure.<br>
<br>
<br>



</li>
</ul>
<h3 id="Compare-the-model-performance-by-ten-fold-cross-validation-and-leave-one-out-method-in-model-evaluation"><a href="#Compare-the-model-performance-by-ten-fold-cross-validation-and-leave-one-out-method-in-model-evaluation" class="headerlink" title="Compare the model performance by ten-fold cross validation and leave-one-out method in model evaluation"></a>Compare the model performance by ten-fold cross validation and leave-one-out method in model evaluation</h3><br>

<p>Owing to the fact that the dataset I used is a small one, only containing 299 samples in total. In order to minimize the bias or error leading by accident, I used ten-fold cross validation and leave-one-out method to return a more general evaluation (calculating the mean value)</p>
<h4 id="Ten-fold-cross-validation-of-SVM"><a href="#Ten-fold-cross-validation-of-SVM" class="headerlink" title="Ten-fold cross validation of SVM"></a>Ten-fold cross validation of SVM</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">heart_data = df.copy()</span><br><span class="line">X = heart_data.iloc[:, <span class="number">0</span>:<span class="number">11</span>]</span><br><span class="line">y = heart_data[<span class="string">&#x27;DEATH_EVENT&#x27;</span>]</span><br><span class="line"></span><br><span class="line">selected_feature = [<span class="string">&#x27;serum_creatinine&#x27;</span>,<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;ejection_fraction&#x27;</span>,<span class="string">&#x27;creatinine_phosphokinase&#x27;</span>]</span><br><span class="line">X_processed = X[selected_feature]</span><br><span class="line">X_processed = StandardScaler().fit_transform(X_processed)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">sv_clf = SVC(kernel= <span class="string">&#x27;linear&#x27;</span>,random_state=<span class="number">1</span>)</span><br><span class="line">sv_clf_rbf = SVC(kernel=<span class="string">&#x27;rbf&#x27;</span>,random_state=<span class="number">1</span>)</span><br><span class="line">scores = cross_val_score(sv_clf, X_processed,y,cv=<span class="number">10</span>)</span><br><span class="line">scores_rbf = cross_val_score(sv_clf_rbf, X_processed,y,cv=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;Ten-fold cross validation scores of linear SVM:&#123;:.3f&#125; &quot;</span>.<span class="built_in">format</span>(np.mean(scores)))</span><br><span class="line">print(<span class="string">&quot;Ten-fold cross validation scores of RBF kernel SVM:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(np.mean(scores_rbf)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>Ten-fold cross validation scores of linear SVM:0.756 
Ten-fold cross validation scores of RBF kernel SVM:0.763</code></pre>
<h4 id="Leave-on-out-method-of-SVM"><a href="#Leave-on-out-method-of-SVM" class="headerlink" title="Leave-on-out method of SVM"></a>Leave-on-out method of SVM</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> LeaveOneOut</span><br><span class="line">loo = LeaveOneOut()</span><br><span class="line">scores_loo = cross_val_score(sv_clf,X_processed,y,cv=loo)</span><br><span class="line">scores_loo_rbf = cross_val_score(sv_clf_rbf,X_processed,y,cv=loo)</span><br><span class="line">print(<span class="string">&quot;Number of CV iterations: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(scores_loo)))</span><br><span class="line">print(<span class="string">&quot;Leave one out method mean scores for linear SVM:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(scores_loo.mean()))</span><br><span class="line">print(<span class="string">&quot;Leave one out method mean scores for RBF SVM:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(scores_loo_rbf.mean()))</span><br></pre></td></tr></table></figure>
<pre><code>Number of CV iterations: 299
Leave one out method mean scores for linear SVM:0.763
Leave one out method mean scores for RBF SVM:0.756</code></pre>
<h4 id="Comparison-of-the-two-methods"><a href="#Comparison-of-the-two-methods" class="headerlink" title="Comparison of the two methods"></a>Comparison of the two methods</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">&quot;Ten-fold cross validation scores of linear SVM:&#123;:.3f&#125; &quot;</span>.<span class="built_in">format</span>(np.mean(scores)))</span><br><span class="line">print(<span class="string">&quot;Ten-fold cross validation scores of RBF kernel SVM:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(np.mean(scores_rbf)))</span><br><span class="line">print(<span class="string">&quot;Leave one out method mean scores for linear SVM:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(scores_loo.mean()))</span><br><span class="line">print(<span class="string">&quot;Leave one out method mean scores for RBF SVM:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(scores_loo_rbf.mean()))</span><br></pre></td></tr></table></figure>
<pre><code>Ten-fold cross validation scores of linear SVM:0.756 
Ten-fold cross validation scores of RBF kernel SVM:0.763
Leave one out method mean scores for linear SVM:0.763
Leave one out method mean scores for RBF SVM:0.756</code></pre>
<p>**Summary: **<br>The results of either ten-fold cross validation and leave-one-out method results are quite similar, based on the consideration of time elapse, I would use ten-fold cross-validation for the following analysis</p>
<h3 id="Hyperparameters-tuning"><a href="#Hyperparameters-tuning" class="headerlink" title="Hyperparameters tuning"></a>Hyperparameters tuning</h3><ul>
<li>Grid search for SVM algorithm with ten-fold cross validation method</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line">param_grid = [&#123;<span class="string">&#x27;kernel&#x27;</span>: [<span class="string">&#x27;rbf&#x27;</span>],</span><br><span class="line">               <span class="string">&#x27;C&#x27;</span>: [<span class="number">0.01</span>,<span class="number">0.1</span>,<span class="number">1</span>,<span class="number">10</span>,<span class="number">100</span>],</span><br><span class="line">               <span class="string">&#x27;gamma&#x27;</span>:[<span class="number">0.01</span>,<span class="number">0.1</span>,<span class="number">1</span>,<span class="number">10</span>,<span class="number">100</span>]&#125;,</span><br><span class="line">                &#123;<span class="string">&#x27;kernel&#x27;</span>:[<span class="string">&#x27;linear&#x27;</span>],</span><br><span class="line">               <span class="string">&#x27;C&#x27;</span>: [<span class="number">0.01</span>,<span class="number">0.1</span>,<span class="number">1</span>,<span class="number">10</span>,<span class="number">100</span>]</span><br><span class="line">                &#125;]</span><br><span class="line"><span class="comment"># print(param_grid)</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=<span class="number">0.2</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">grid_search = GridSearchCV(SVC(random_state=<span class="number">1</span>),param_grid,cv =<span class="number">10</span>, n_jobs=-<span class="number">1</span>)</span><br><span class="line">grid_search.fit(X_train,y_train)</span><br><span class="line">print(<span class="string">&quot;Best cross validation accuracy: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(grid_search.best_score_))</span><br><span class="line">print(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(grid_search.score(X_test,y_test)))</span><br><span class="line">print(<span class="string">&quot;Best parameters: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(grid_search.best_params_))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>Best cross validation accuracy: 0.75
Test set score: 0.87
Best parameters: &#123;&#39;C&#39;: 1, &#39;gamma&#39;: 0.1, &#39;kernel&#39;: &#39;rbf&#39;&#125;</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">results = pd.DataFrame(grid_search.cv_results_)</span><br><span class="line">results_rbf = results.iloc[<span class="number">0</span>:<span class="number">25</span>,:]</span><br><span class="line">results_linear = results.iloc[<span class="number">25</span>:,:]</span><br><span class="line"><span class="comment"># display(results_linear.T.head())</span></span><br><span class="line"><span class="comment"># display(results_rbf.T.head())</span></span><br><span class="line"><span class="comment"># display(results.T.head())</span></span><br><span class="line"><span class="comment"># display(results.T)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">12</span>))</span><br><span class="line">scores = np.array(results_rbf.mean_test_score).reshape(<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">heatmap = sn.heatmap(data=scores,xticklabels=param_grid[<span class="number">0</span>][<span class="string">&#x27;gamma&#x27;</span>],</span><br><span class="line">           yticklabels=param_grid[<span class="number">0</span>][<span class="string">&#x27;C&#x27;</span>],</span><br><span class="line">           annot=<span class="literal">True</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;C&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;gamma&quot;</span>)</span><br></pre></td></tr></table></figure>



<pre><code>Text(0.5, 87.0, &#39;gamma&#39;)</code></pre>
<p><img src="http://i2.tiimg.com/732634/7fbdc5dcac6a34a6.png" alt="Markdown"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sv_clf_rbf = SVC(kernel=<span class="string">&#x27;rbf&#x27;</span>,random_state=<span class="number">1</span>,C=<span class="number">1</span>, gamma = <span class="number">0.1</span>)</span><br><span class="line">scores_loo_rbf = cross_val_score(sv_clf_rbf,X_processed,y,cv=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># print(&quot;Number of CV iterations: &#123;&#125;&quot;.format(len(scores_loo)))</span></span><br><span class="line">print(<span class="string">&quot;10-fold cross validation mean method scores for RBF SVM:&#123;:.3f&#125;&quot;</span>.<span class="built_in">format</span>(scores_loo_rbf.mean()))</span><br></pre></td></tr></table></figure>
<pre><code>10-fold cross validation mean method scores for RBF SVM:0.763</code></pre>
<p>As can be seen from the grid search, the SVM with RBF kernel and hyperparameter C: 1, gamma 0.1 could achieve the highest performance</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">linear_score = np.array(results_linear.mean_test_score)</span><br><span class="line">display(results_linear.T.head())</span><br><span class="line">print(<span class="built_in">max</span>(linear_score))</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>25</th>
      <th>26</th>
      <th>27</th>
      <th>28</th>
      <th>29</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>mean_fit_time</th>
      <td>0.0021414</td>
      <td>0.00199142</td>
      <td>0.00196731</td>
      <td>0.00795052</td>
      <td>0.0434071</td>
    </tr>
    <tr>
      <th>std_fit_time</th>
      <td>0.000204684</td>
      <td>0.0003023</td>
      <td>0.000284892</td>
      <td>0.00302314</td>
      <td>0.010914</td>
    </tr>
    <tr>
      <th>mean_score_time</th>
      <td>0.000635695</td>
      <td>0.000739121</td>
      <td>0.000567961</td>
      <td>0.000703764</td>
      <td>0.000822139</td>
    </tr>
    <tr>
      <th>std_score_time</th>
      <td>0.000104189</td>
      <td>0.000289296</td>
      <td>9.88127e-05</td>
      <td>8.55103e-05</td>
      <td>0.000257763</td>
    </tr>
    <tr>
      <th>param_C</th>
      <td>0.01</td>
      <td>0.1</td>
      <td>1</td>
      <td>10</td>
      <td>100</td>
    </tr>
  </tbody>
</table>
</div>


<pre><code>0.7358695652173913</code></pre>
<p>In the meantime, the best performance in linear model is 0.73587, and the mean_test_score in c =1,10,100 is relative similar. The influence of hyperparameter C are not significant. Apart from that the SVC function only provide</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">### Hyperparameter tuning for linear SVM (penalty, loss, C)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">parameters = &#123;<span class="string">&#x27;penalty&#x27;</span>:[<span class="string">&#x27;l1&#x27;</span>, <span class="string">&#x27;l2&#x27;</span>], <span class="string">&#x27;loss&#x27;</span>:[<span class="string">&#x27;hinge&#x27;</span>, <span class="string">&#x27;squared_hinge&#x27;</span>],<span class="string">&#x27;C&#x27;</span>:[<span class="number">1</span>, <span class="number">10</span>,<span class="number">100</span>]&#125;</span><br><span class="line">grid_search = GridSearchCV(LinearSVC(random_state=<span class="number">1</span>),parameters,cv =<span class="number">10</span>, n_jobs=-<span class="number">1</span>)</span><br><span class="line">grid_search.fit(X_train,y_train)</span><br><span class="line">print(<span class="string">&quot;Best cross validation accuracy: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(grid_search.best_score_))</span><br><span class="line">print(<span class="string">&quot;Test set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(grid_search.score(X_test,y_test)))</span><br><span class="line">print(<span class="string">&quot;Best parameters: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(grid_search.best_params_))</span><br></pre></td></tr></table></figure>
<pre><code>Best cross validation accuracy: 0.75
Test set score: 0.80
Best parameters: &#123;&#39;C&#39;: 10, &#39;loss&#39;: &#39;squared_hinge&#39;, &#39;penalty&#39;: &#39;l2&#39;&#125;</code></pre>
<p>The test score of linear svm (test set score:0.80) still could not compete with the rbf kernel (test set score: 0.87), therefore, I would still insist on the SVM with RBF kernel and hyperparameter C: 1, gamma 0.1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> plot_confusion_matrix, confusion_matrix</span><br><span class="line">heart_data = df.copy()</span><br><span class="line"></span><br><span class="line">X = heart_data.iloc[:, <span class="number">0</span>:<span class="number">11</span>]</span><br><span class="line">y = heart_data[<span class="string">&#x27;DEATH_EVENT&#x27;</span>]</span><br><span class="line"></span><br><span class="line">selected_feature = [<span class="string">&#x27;serum_creatinine&#x27;</span>,<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;ejection_fraction&#x27;</span>,<span class="string">&#x27;creatinine_phosphokinase&#x27;</span>]</span><br><span class="line">X_processed = X[selected_feature]</span><br><span class="line">X_processed = StandardScaler().fit_transform(X_processed)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_processed,y,test_size=<span class="number">0.2</span>,random_state=<span class="number">2</span>)</span><br><span class="line">model = SVC(kernel=<span class="string">&#x27;rbf&#x27;</span>,random_state=<span class="number">1</span>,C=<span class="number">1</span>, gamma = <span class="number">0.1</span>)</span><br><span class="line">model.fit(X_train,y_train)</span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line">y_pred_train = model.predict(X_train)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="Training-and-validation-loss"><a href="#Training-and-validation-loss" class="headerlink" title="Training and validation loss"></a>Training and validation loss</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">&quot;Training loss is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">round</span>(model.score(X_train,y_train),<span class="number">2</span>)))</span><br><span class="line">print(<span class="string">&quot;Testing loss is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">round</span>(model.score(X_test,y_test),<span class="number">2</span>)))</span><br></pre></td></tr></table></figure>
<pre><code>Training loss is: 0.78
Testing loss is: 0.83</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_mse = mean_squared_error(y_train,y_pred_train)</span><br><span class="line">test_mse = mean_squared_error(y_test,y_pred)</span><br><span class="line">print(<span class="string">&quot;The MSE value of training sample is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">round</span>(train_mse,<span class="number">2</span>)))</span><br><span class="line">print(<span class="string">&quot;The MSE value of testing sample is: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">round</span>(test_mse,<span class="number">2</span>)))</span><br></pre></td></tr></table></figure>
<pre><code>The MSE value of training sample is: 0.22
The MSE value of testing sample is: 0.17</code></pre>
<p>The training loss in SVM with RBF kernel in hyperparameter C: 1, gamma 0.1 is 0.78, with testing loss equals 0.83<br>The MSE value for training and testing in SVM with RBF kernel is 0.22 and 0.17 accordingly</p>
<h3 id="Plot-the-confusion-matrix-plot"><a href="#Plot-the-confusion-matrix-plot" class="headerlink" title="Plot the confusion matrix plot"></a>Plot the confusion matrix plot</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># confusion = confusion_matrix(y_test,y_pred)</span></span><br><span class="line">plot_confusion_matrix(model,X_test,y_test)</span><br><span class="line">plt.title(<span class="string">&quot;SVC Model - Confusion Matrix&quot;</span>)</span><br><span class="line">plt.xticks(<span class="built_in">range</span>(<span class="number">2</span>), [<span class="string">&quot;Alive&quot;</span>, <span class="string">&quot;Dead&quot;</span>], fontsize=<span class="number">8</span>)</span><br><span class="line">plt.yticks(<span class="built_in">range</span>(<span class="number">2</span>), [<span class="string">&quot;Alive&quot;</span>, <span class="string">&quot;Dead&quot;</span>], fontsize=<span class="number">8</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://i2.tiimg.com/732634/2407f6defc2653a5.png" alt="Markdown"></p>
<h3 id="Evaluate-the-performance-of-the-final-selected-model-performance-with-respect-to-the-chosen-performance-matrix"><a href="#Evaluate-the-performance-of-the-final-selected-model-performance-with-respect-to-the-chosen-performance-matrix" class="headerlink" title="Evaluate the performance of the final selected model performance with respect to the chosen performance matrix"></a>Evaluate the performance of the final selected model performance with respect to the chosen performance matrix</h3><br>

<h3 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy:"></a>Accuracy:</h3><p>$$\frac{TP+TN}{TP+TN+FP+FN}$$<br><br></p>
<h3 id="Total"><a href="#Total" class="headerlink" title="Total:"></a>Total:</h3><p>$$Total = TP+TN+FP+FN$$</p>
<h3 id="Precision"><a href="#Precision" class="headerlink" title="Precision:"></a>Precision:</h3><p>$$\frac{TP}{TP+FP}$$</p>
<h3 id="Recall"><a href="#Recall" class="headerlink" title="Recall:"></a>Recall:</h3><p>$$\frac{TP}{TP+FN}$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">print(classification_report(y_test,y_pred,target_names=[<span class="string">&quot;Heart Not Failed&quot;</span>, <span class="string">&quot;Heart Fail&quot;</span>]))</span><br></pre></td></tr></table></figure>
<pre><code>                  precision    recall  f1-score   support

Heart Not Failed       0.85      0.93      0.89        43
      Heart Fail       0.77      0.59      0.67        17

        accuracy                           0.83        60
       macro avg       0.81      0.76      0.78        60
    weighted avg       0.83      0.83      0.83        60</code></pre>
<h3 id="ROC-curve"><a href="#ROC-curve" class="headerlink" title="ROC curve"></a>ROC curve</h3><p>AUC = Area under the curve</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve</span><br><span class="line">plot_roc_curve(model,X_test,y_test)</span><br></pre></td></tr></table></figure>



<pre><code>&lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7fdff1de91f0&gt;</code></pre>
<p><img src="http://i2.tiimg.com/732634/45a09aaa074d900e.png" alt="Markdown"></p>
<h3 id="PR-curve"><a href="#PR-curve" class="headerlink" title="PR-curve"></a>PR-curve</h3><p>AP = Average Precision, summarizes such a plot as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:</p>
<p>$$ AP =\sum_{N}(R_n-R_{n-1})P_n$$</p>
<p>where  and  are the precision and recall at the nth threshold. A pair  is referred to as an operating point.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_precision_recall_curve(model,X_test,y_test)</span><br></pre></td></tr></table></figure>

<p><img src="http://i2.tiimg.com/732634/0f1422e7de2481d4.png" alt="Markdown"></p>
<h3 id="Summary-of-the-results"><a href="#Summary-of-the-results" class="headerlink" title="Summary of the results"></a>Summary of the results</h3><ul>
<li>Grid search to find the best hyperparameters in optimization of the model</li>
<li>ten-fold cross validation used to evaluate machine learning models on a limited data sample</li>
<li>SVM with RBF kernel and hyperparameter C: 1, gamma 0.1 would be the best performed model</li>
<li>precision, recall, f1-score, accuracy value were applied and summarized in the previous table</li>
<li>train loss and validation loss were 0.78 and 0.83 accordingly</li>
<li>MSE value of training sample and testing sample is 0.22 and 0.17 accordingly</li>
<li>No overfitting or underfitting problem observed because regularization method, cross-validation method were used to mitigate the potential problems</li>
<li>The overall model is robust and in good generalization ability</li>
<li>ROC curve and PR curve were visualized to demonstrate the final results; AUC is 0.83 and AP is 0.7</li>
</ul>
<h2 id="Conclusion-and-future-work"><a href="#Conclusion-and-future-work" class="headerlink" title="Conclusion and future work"></a>Conclusion and future work</h2><p>In this project, I compared and devised a RBF SVM machine learning model for the prediction of heart failure. I firstly conducted a preliminary analysis of the whole dataset and utilized the PCA method to visualize the distribution condition of the samples. The initial analysis was not robust and precise, which required me to optimize the model. Then, I conducted feature selection to return the top 4 most correlated feature with the death event. Apart from that, I also compared the difference between normalization method as well as the validation methods. The final determined methods were to use four selected features (‘serum_creatinine’,’age’,’ejection_fraction’, ‘creatinine_phosphokinase’), StandardScalar methods and ten-fold cross validation to conduct the prediction. In addition, I also conduct the explorative data analysis to have a view of the selected feature. Furthermore, the PCA condition after feature selection still could not determine whether the dataset is linearly seperable or not, I, therefore, used the grid serach to return the most suitable hyperparameters. The final results suggested that the SVM with RBF kernel in gamma equals 0.1 and penalty C equals 1 could achieve the best performance. After that, the model performance was evaluated from confusion matrix, accuracy, precision, recall, f1-score. Moreover, the ROC plot and PR-curve plot with AUC and AP accordingly were indicated. The final accuracy could achieve 0.83, which is satisfactory.</p>
<p>In the future, since this project only used SVM related algorithms, I would like to incorporate more complicate algorithms, for instance, XGboost. Furthermore, the model’s interpretability are not great enough, and that would be my focus in the future.</p>
<h2 id="Bibliography"><a href="#Bibliography" class="headerlink" title="Bibliography"></a>Bibliography</h2><p>Heart Fail:Analysis and Quick-prediction | Kaggle. (n.d.). Retrieved January 8, 2021, from <a class="link"   href="https://www.kaggle.com/nayansakhiya/heart-fail-analysis-and-quick-prediction" >https://www.kaggle.com/nayansakhiya/heart-fail-analysis-and-quick-prediction<i class="fas fa-external-link-alt"></i></a></p>
<p>Heart Failure - Model Prediction Comparisons (95%) | Kaggle. (n.d.). Retrieved January 8, 2021, from <a class="link"   href="https://www.kaggle.com/rude009/heart-failure-model-prediction-comparisons-95" >https://www.kaggle.com/rude009/heart-failure-model-prediction-comparisons-95<i class="fas fa-external-link-alt"></i></a></p>
<p>Heart Failure Prediction (AUC: 0.98) | Kaggle. (n.d.). Retrieved January 8, 2021, from <a class="link"   href="https://www.kaggle.com/ksvmuralidhar/heart-failure-prediction-auc-0-98" >https://www.kaggle.com/ksvmuralidhar/heart-failure-prediction-auc-0-98<i class="fas fa-external-link-alt"></i></a></p>
<p>Pedregosa et al., Scikit-learn: Machine Learning in Python, JMLR 12, pp. 2825-2830, 2011.</p>
<h2 id="Other-resources-to-download-the-code"><a href="#Other-resources-to-download-the-code" class="headerlink" title="Other resources to download the code"></a>Other resources to download the code</h2><p>Kaggle:<br><a class="link"   href="https://www.kaggle.com/yuxuanwu17/eda-of-heart-failure-and-optimization-of-svm" >https://www.kaggle.com/yuxuanwu17/eda-of-heart-failure-and-optimization-of-svm<i class="fas fa-external-link-alt"></i></a></p>
<p>Github:<br><a class="link"   href="https://github.com/yuxuanwu17/kaggle/blob/main/Final_report.ipynb" >https://github.com/yuxuanwu17/kaggle/blob/main/Final_report.ipynb<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <tags>
        <tag>kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>Explorative data anlysis of avocado price from 2015-2020</title>
    <url>/2021/01/25/avocado_pred/</url>
    <content><![CDATA[<p>Avocado, bright green fruit with a large pit and dark leathery skin, has become a fashion among millennials in America. Avocados are featured in their unique flavor, rich texture, and the high percentage of healthy fat, which is an ideal food for the healthy diet. However, the average price for avocados is not cheap, which made those millennials unable to achieve their “Avocado Freedom” dream. In addition, as an applicant in American master degree and a fan of avocados, it is of significant interest for me to conduct the explorative data analysis of avocados, hoping to figure out some hidden patterns, like seasonal patterns or cyclical patterns. Furthermore, related factors were also evaluated to suggest the influence on the avocados’ price, which could help these millennials to find the cheaper avocados in a scientific way. </p>
<h3 id="Import-the-libraries"><a href="#Import-the-libraries" class="headerlink" title="Import the libraries"></a>Import the libraries</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(tidyr)</span><br><span class="line">library(skimr)</span><br><span class="line">library(GGally)</span><br><span class="line">library(viridis)</span><br><span class="line">library(caret)</span><br><span class="line">library(e1071)</span><br><span class="line">library(rpart)</span><br><span class="line">library(xgboost)</span><br><span class="line">library(forecast)</span><br><span class="line">library(corrplot)</span><br><span class="line">library(corrgram)</span><br><span class="line">library(ggplot2)</span><br><span class="line">library(ggthemes)</span><br><span class="line">library(psych)</span><br><span class="line">library(scales)</span><br><span class="line">library(treemap)</span><br><span class="line">library(repr)</span><br><span class="line">library(cowplot)</span><br><span class="line">library(magrittr)</span><br><span class="line">library(ggpubr)</span><br><span class="line">library(RColorBrewer)</span><br><span class="line">library(plotrix)</span><br><span class="line">library(ggrepel)</span><br><span class="line">library(tidyverse)</span><br><span class="line">library(gridExtra)</span><br><span class="line">library(lubridate)</span><br><span class="line">library(tibbletime)</span><br><span class="line">library(reshape2)</span><br><span class="line">library(prophet)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="Load-the-data-and-return-the-head-of-data"><a href="#Load-the-data-and-return-the-head-of-data" class="headerlink" title="Load the data and return the head of data"></a>Load the data and return the head of data</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">df &lt;- read.csv(&quot;&#x2F;Users&#x2F;yuxuan&#x2F;Desktop&#x2F;INT303-Avocado-prediction&#x2F;avocado-updated-2020.csv&quot;)</span><br><span class="line">head(df)</span><br><span class="line">colnames(df)</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">date</th>
<th align="left">average_price</th>
<th align="left">total_volume</th>
<th align="left">X4046</th>
<th align="left">X4225</th>
<th align="left">X4770</th>
<th align="left">total_bags</th>
<th align="left">small_bags</th>
<th align="left">large_bags</th>
<th align="left">xlarge_bags</th>
<th align="left">type</th>
<th align="left">year</th>
<th align="left">geography</th>
</tr>
</thead>
<tbody><tr>
<td align="left"></td>
<td align="left"><fct></td>
<td align="left"><dbl></td>
<td align="left"><dbl></td>
<td align="left"><dbl></td>
<td align="left"><dbl></td>
<td align="left"><dbl></td>
<td align="left"><dbl></td>
<td align="left"><dbl></td>
<td align="left"><dbl></td>
<td align="left"><dbl></td>
<td align="left"><fct></td>
<td align="left"><int></td>
<td align="left"><fct></td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">2015-01-04</td>
<td align="left">1.22</td>
<td align="left">40873.28</td>
<td align="left">2819.50</td>
<td align="left">28287.42</td>
<td align="left">49.90</td>
<td align="left">9716.46</td>
<td align="left">9186.93</td>
<td align="left">529.53</td>
<td align="left">0</td>
<td align="left">conventional</td>
<td align="left">2015</td>
<td align="left">Albany</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">2015-01-04</td>
<td align="left">1.79</td>
<td align="left">1373.95</td>
<td align="left">57.42</td>
<td align="left">153.88</td>
<td align="left">0.00</td>
<td align="left">1162.65</td>
<td align="left">1162.65</td>
<td align="left">0.00</td>
<td align="left">0</td>
<td align="left">organic</td>
<td align="left">2015</td>
<td align="left">Albany</td>
</tr>
<tr>
<td align="left">3</td>
<td align="left">2015-01-04</td>
<td align="left">1.00</td>
<td align="left">435021.49</td>
<td align="left">364302.39</td>
<td align="left">23821.16</td>
<td align="left">82.15</td>
<td align="left">46815.79</td>
<td align="left">16707.15</td>
<td align="left">30108.64</td>
<td align="left">0</td>
<td align="left">conventional</td>
<td align="left">2015</td>
<td align="left">Atlanta</td>
</tr>
<tr>
<td align="left">4</td>
<td align="left">2015-01-04</td>
<td align="left">1.76</td>
<td align="left">3846.69</td>
<td align="left">1500.15</td>
<td align="left">938.35</td>
<td align="left">0.00</td>
<td align="left">1408.19</td>
<td align="left">1071.35</td>
<td align="left">336.84</td>
<td align="left">0</td>
<td align="left">organic</td>
<td align="left">2015</td>
<td align="left">Atlanta</td>
</tr>
<tr>
<td align="left">5</td>
<td align="left">2015-01-04</td>
<td align="left">1.08</td>
<td align="left">788025.06</td>
<td align="left">53987.31</td>
<td align="left">552906.04</td>
<td align="left">39995.03</td>
<td align="left">141136.68</td>
<td align="left">137146.07</td>
<td align="left">3990.61</td>
<td align="left">0</td>
<td align="left">conventional</td>
<td align="left">2015</td>
<td align="left">Baltimore/Washington</td>
</tr>
<tr>
<td align="left">6</td>
<td align="left">2015-01-04</td>
<td align="left">1.29</td>
<td align="left">19137.28</td>
<td align="left">8040.64</td>
<td align="left">6557.47</td>
<td align="left">657.48</td>
<td align="left">3881.69</td>
<td align="left">3881.69</td>
<td align="left">0.00</td>
<td align="left">0</td>
<td align="left">organic</td>
<td align="left">2015</td>
<td align="left">Baltimore/Washington</td>
</tr>
</tbody></table>
<h3 id="Check-whether-the-dataset-contains-the-missing-value"><a href="#Check-whether-the-dataset-contains-the-missing-value" class="headerlink" title="Check whether the dataset contains the missing value"></a>Check whether the dataset contains the missing value</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sum(is.na(df))</span><br></pre></td></tr></table></figure>
<p>The overall dataset do not contain any missing value</p>
<h3 id="Explore-the-data-and-some-clarification"><a href="#Explore-the-data-and-some-clarification" class="headerlink" title="Explore the data and some clarification"></a>Explore the data and some clarification</h3><h4 id="Explain-the-features"><a href="#Explain-the-features" class="headerlink" title="Explain the features"></a>Explain the features</h4><ul>
<li>date - The date of the observation</li>
<li>average_price - The average price of a single</li>
<li>total_volume - Total number of avocados sold</li>
<li>year - The year</li>
<li>type - conventional or organic</li>
<li>geography - The city or region of the observation</li>
</ul>
<h4 id="X4046-X4225-X4770-stands-for-the-PLU-code"><a href="#X4046-X4225-X4770-stands-for-the-PLU-code" class="headerlink" title="X4046, X4225, X4770 stands for the PLU code"></a>X4046, X4225, X4770 stands for the PLU code</h4><ul>
<li>Small/Medium Hass Avocado (~3-5oz avocado) | #4046 </li>
<li>Large Hass Avocado (~8-10oz avocado) | #4225 </li>
<li>Extra Large Hass Avocado (~10-15oz avocado) | #4770 </li>
</ul>
<h3 id="Exploratory-Data-Analysis"><a href="#Exploratory-Data-Analysis" class="headerlink" title="Exploratory Data Analysis"></a>Exploratory Data Analysis</h3><h4 id="Density-plot-of-the-difference-between-two-avocados"><a href="#Density-plot-of-the-difference-between-two-avocados" class="headerlink" title="Density plot of the difference between two avocados."></a>Density plot of the difference between two avocados.</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">levels(df$type)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(ggplot2)</span><br><span class="line">options(repr.plot.width &#x3D; 8, repr.plot.height &#x3D; 4)</span><br><span class="line">density_plot &lt;- ggplot(df, aes(x&#x3D;average_price, fill&#x3D;type))+</span><br><span class="line">  geom_density()+</span><br><span class="line">  facet_wrap(~type)+</span><br><span class="line">  theme_minimal()+</span><br><span class="line">  theme(plot.title &#x3D; element_text(hjust &#x3D; 0.5),legend.position &#x3D; &quot;bottom&quot;)+</span><br><span class="line">  labs(title &#x3D; &quot;Avocado Price by type&quot;)+</span><br><span class="line">  scale_fill_brewer(palette &#x3D; &quot;Set2&quot;)</span><br><span class="line">density_plot</span><br></pre></td></tr></table></figure>
<p><img src="https://www.kaggleusercontent.com/kf/49257785/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..tZfBZbcjjkqxj4RmQEJoyw.A2C4X-_8qnKSkIVucnomHICENKUmjTPXUgT1XqFjrKp8Vwfpyt-nJXhYhke8yzjlDiL_FT1jvmw9GnBDuF_MeWzQUxn59txsXKEQ5TNxy2sZy6GYoHD4yeJb96CHu7tEvhTmc3Pf1nseFJUmMjAKCktdygT-uHDSuVQKAPNnGUpFNmn5fx_6OsujxRZO9TdXdWiqH1GGat_EqgxwZW-_o856VcxtNXe5WenELwGdFrP_fHsmRkvvcjtEmEXzl2_blPw6GMAdMmDD7WsNhKVd4zJoOVKvPx1awtn9623tYAqFfDJ3qzKqLfFAhNRAMsqQ-zfIos2ndjfK3YTRnRTKn_TX9TkPBnbni_OR6teuY24PW_hMZpFYW332SdtN2sCHyHke6Oaqgs6uq8l6jX14yyIMEhgiWkl5Usn36w1aLLtP4YQZIjO1m7B9FsVSOFeULVuVmThf6LyKuvsoG3N568gEqTKblQvj-_R0hUqGhc6W-3DZyE0hBoqgO__LBxbYYZ51DdMf-2bv2Hj99gNwuS-EVb1GFv__UrplkB63GGn-ftucHSQBkxSDoKlB0F99ZiGNWL4cmii009OqgG_mJQI9W-iqgB6sW3k3T3RdVL3QpdG-D3XEhYgfQTcMV6P23-9iQKNgOcTD1CWwWPVjsJfgu_NZHVIoUScEs9mdivE.1eUtwBW-D1O5QgrEgVmjhw/__results___files/__results___14_1.png" alt="img"></p>
<ol>
<li><h3 id="Create-a-matrix-to-demonstrate-the-volume-of-conventional-and-organic-avocados"><a href="#Create-a-matrix-to-demonstrate-the-volume-of-conventional-and-organic-avocados" class="headerlink" title="Create a matrix to demonstrate the volume of conventional and organic avocados"></a>Create a matrix to demonstrate the volume of conventional and organic avocados</h3></li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(dplyr)</span><br><span class="line">vol_type &lt;- df %&gt;% group_by(type) %&gt;% summarise(average_volume &#x3D; round(mean(total_volume),3),average_price &#x3D; round(mean(average_price),3)) %&gt;% mutate(volume_percent&#x3D; round(prop.table(average_volume)*100,3))</span><br><span class="line">vol_type</span><br><span class="line"></span><br><span class="line">#plu_conv &lt;- df %&gt;% select(type,total_volume,X4046,X4225,X4770) %&gt;% group_by(type) %&gt;% summarise(average_volume &#x3D; round(mean(total_volume),3),x4046 &#x3D; sum(X4046),x4225 &#x3D; sum(X4225),x4770 &#x3D; sum(X4770)) %&gt;% mutate(volume_percent&#x3D; round(prop.table(average_volume)*100,3))</span><br><span class="line">#plu_conv</span><br><span class="line"></span><br><span class="line">#cor(df$average_price,df$total_volume)</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th align="center">type</th>
<th align="left">average_volume</th>
<th align="left">average_price</th>
<th align="left">volume_percent</th>
</tr>
</thead>
<tbody><tr>
<td align="center">organic</td>
<td align="left">60127.48</td>
<td align="left">1.624</td>
<td align="left">3.201%</td>
</tr>
<tr>
<td align="center">conventional</td>
<td align="left">1818206.02</td>
<td align="left">1.158</td>
<td align="left">96.799%</td>
</tr>
</tbody></table>
<p><strong>As can be seen from the density plot and the table in avocados.</strong> </p>
<ul>
<li>there are two types of avocado: organic and conventional  </li>
<li>organic avocado share a small percent (3.2%) of volume but has a high price (1.62)  </li>
<li>conventional avocado share a large percent (96.8) of volume but has a relative low price (1.16) </li>
</ul>
<h3 id="Compare-the-volume-of-each-avocado"><a href="#Compare-the-volume-of-each-avocado" class="headerlink" title="Compare the volume of each avocado"></a>Compare the volume of each avocado</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x4770 &lt;- df$X4770 %&gt;% sum()</span><br><span class="line">x4046 &lt;- df$X4046 %&gt;% sum()</span><br><span class="line">x4225 &lt;- df$X4225 %&gt;% sum()</span><br><span class="line">total_types &lt;- x4770+x4046+x4225</span><br><span class="line">total_types</span><br><span class="line">df$total_volume %&gt;% sum()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[1] &quot;The sum of volume in x4770, x4046, x4225 is:  18181860558.8&quot;</span><br><span class="line">[1] &quot;The sum of volume in the sum of total_volume:  28197362107.92&quot;</span><br></pre></td></tr></table></figure>
<p>The reason causing this difference is that there are different kinds of avocados, this dataset only consider the plu code of Hass avocados, it is obvious that it will cause the difference</p>
<h3 id="Avocado-price-from-2015-2020"><a href="#Avocado-price-from-2015-2020" class="headerlink" title="Avocado price from 2015-2020"></a>Avocado price from 2015-2020</h3><figure class="highlight plain"><figcaption><span>ggplot&#125;</span></figcaption><table><tr><td class="code"><pre><span class="line">library(ggplot2)</span><br><span class="line">## Change the Date column from factor to the date format</span><br><span class="line">df$date &lt;- as.Date(df$date, &quot;%Y-%m-%d&quot;)</span><br><span class="line"></span><br><span class="line">## Sort the dates and order the datesets in date</span><br><span class="line">df &lt;- df[order(df$date),]</span><br><span class="line"></span><br><span class="line">## Make the plot</span><br><span class="line">comparision_plot &lt;- df %&gt;% select(date, average_price, type) %&gt;%</span><br><span class="line">  ggplot(aes(x&#x3D;date,y&#x3D;average_price))+</span><br><span class="line">  geom_area(aes(color&#x3D;type,fill&#x3D;type),alpha&#x3D;0.3,position&#x3D;position_dodge(0.8))+</span><br><span class="line">  theme_bw()+</span><br><span class="line">  scale_color_manual(values &#x3D; c(&quot;#ED7921&quot;,&quot;#62BE51&quot;))+</span><br><span class="line">  scale_fill_manual(values &#x3D; c(&quot;#FD833E&quot;,&quot;#B8FC5F&quot;)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">comparision_plot</span><br><span class="line">#plot_grid(density_plot,comparision,ncol &#x3D; 2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://www.kaggleusercontent.com/kf/49257785/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..tZfBZbcjjkqxj4RmQEJoyw.A2C4X-_8qnKSkIVucnomHICENKUmjTPXUgT1XqFjrKp8Vwfpyt-nJXhYhke8yzjlDiL_FT1jvmw9GnBDuF_MeWzQUxn59txsXKEQ5TNxy2sZy6GYoHD4yeJb96CHu7tEvhTmc3Pf1nseFJUmMjAKCktdygT-uHDSuVQKAPNnGUpFNmn5fx_6OsujxRZO9TdXdWiqH1GGat_EqgxwZW-_o856VcxtNXe5WenELwGdFrP_fHsmRkvvcjtEmEXzl2_blPw6GMAdMmDD7WsNhKVd4zJoOVKvPx1awtn9623tYAqFfDJ3qzKqLfFAhNRAMsqQ-zfIos2ndjfK3YTRnRTKn_TX9TkPBnbni_OR6teuY24PW_hMZpFYW332SdtN2sCHyHke6Oaqgs6uq8l6jX14yyIMEhgiWkl5Usn36w1aLLtP4YQZIjO1m7B9FsVSOFeULVuVmThf6LyKuvsoG3N568gEqTKblQvj-_R0hUqGhc6W-3DZyE0hBoqgO__LBxbYYZ51DdMf-2bv2Hj99gNwuS-EVb1GFv__UrplkB63GGn-ftucHSQBkxSDoKlB0F99ZiGNWL4cmii009OqgG_mJQI9W-iqgB6sW3k3T3RdVL3QpdG-D3XEhYgfQTcMV6P23-9iQKNgOcTD1CWwWPVjsJfgu_NZHVIoUScEs9mdivE.1eUtwBW-D1O5QgrEgVmjhw/__results___files/__results___22_0.png" alt="img"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ggplot(data&#x3D;df, aes(x&#x3D;date, y&#x3D;average_price,col&#x3D;type))+</span><br><span class="line">  geom_line()+</span><br><span class="line">  facet_wrap(~ type)+</span><br><span class="line">  theme_bw()+</span><br><span class="line">  theme(legend.position &#x3D; &quot;position&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="https://www.kaggleusercontent.com/kf/49257785/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..tZfBZbcjjkqxj4RmQEJoyw.A2C4X-_8qnKSkIVucnomHICENKUmjTPXUgT1XqFjrKp8Vwfpyt-nJXhYhke8yzjlDiL_FT1jvmw9GnBDuF_MeWzQUxn59txsXKEQ5TNxy2sZy6GYoHD4yeJb96CHu7tEvhTmc3Pf1nseFJUmMjAKCktdygT-uHDSuVQKAPNnGUpFNmn5fx_6OsujxRZO9TdXdWiqH1GGat_EqgxwZW-_o856VcxtNXe5WenELwGdFrP_fHsmRkvvcjtEmEXzl2_blPw6GMAdMmDD7WsNhKVd4zJoOVKvPx1awtn9623tYAqFfDJ3qzKqLfFAhNRAMsqQ-zfIos2ndjfK3YTRnRTKn_TX9TkPBnbni_OR6teuY24PW_hMZpFYW332SdtN2sCHyHke6Oaqgs6uq8l6jX14yyIMEhgiWkl5Usn36w1aLLtP4YQZIjO1m7B9FsVSOFeULVuVmThf6LyKuvsoG3N568gEqTKblQvj-_R0hUqGhc6W-3DZyE0hBoqgO__LBxbYYZ51DdMf-2bv2Hj99gNwuS-EVb1GFv__UrplkB63GGn-ftucHSQBkxSDoKlB0F99ZiGNWL4cmii009OqgG_mJQI9W-iqgB6sW3k3T3RdVL3QpdG-D3XEhYgfQTcMV6P23-9iQKNgOcTD1CWwWPVjsJfgu_NZHVIoUScEs9mdivE.1eUtwBW-D1O5QgrEgVmjhw/__results___files/__results___23_0.png" alt="img"></p>
<ul>
<li>The price of organic avocados are always higher than normal</li>
<li>The price of both avocados follow some seasonal patterns, which obeyed the commob view</li>
<li>Whether the relationship between the volume and price existed need to be explored in the following part</li>
</ul>
<h3 id="Relationship-between-Prices-and-Total-on-either-conventional-or-organic-avocados"><a href="#Relationship-between-Prices-and-Total-on-either-conventional-or-organic-avocados" class="headerlink" title="Relationship between Prices and Total on either conventional or organic avocados"></a>Relationship between Prices and Total on either conventional or organic avocados</h3><h4 id="Filter-the-data-into-two-categories-conventional-or-organic"><a href="#Filter-the-data-into-two-categories-conventional-or-organic" class="headerlink" title="Filter the data into two categories, conventional or organic"></a>Filter the data into two categories, conventional or organic</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">organic &lt;- df %&gt;% select(type,average_price,total_volume,date) %&gt;% filter(type&#x3D;&#x3D;&quot;organic&quot;)</span><br><span class="line">#head(organic)</span><br><span class="line">conventional &lt;- df %&gt;% select(type,average_price,total_volume,date) %&gt;% filter(type&#x3D;&#x3D;&quot;conventional&quot;)</span><br><span class="line">#head(conventional)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>tribble_tabel&#125;</span></figcaption><table><tr><td class="code"><pre><span class="line">library(tibbletime)</span><br><span class="line">organic &lt;- as_tbl_time(organic,index &#x3D; date) %&gt;% as_period(&#39;1 month&#39;)</span><br><span class="line">conventional &lt;- as_tbl_time(conventional,index &#x3D; date) %&gt;% as_period(&#39;monthly&#39;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="Monthly-avocados-price-in-either-conventional-or-organic-avocados"><a href="#Monthly-avocados-price-in-either-conventional-or-organic-avocados" class="headerlink" title="Monthly avocados price in either conventional or organic avocados"></a>Monthly avocados price in either conventional or organic avocados</h4><figure class="highlight plain"><figcaption><span>cowplot, fig.width </span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">library(ggplot2)</span><br><span class="line">library(ggthemes)</span><br><span class="line">library(cowplot)</span><br><span class="line"></span><br><span class="line">options(repr.plot.width&#x3D;8, repr.plot.height&#x3D;6)</span><br><span class="line"></span><br><span class="line">## average-price with time series</span><br><span class="line">conventional_monthly &lt;- conventional %&gt;%</span><br><span class="line">    ggplot(aes(x&#x3D;date,y&#x3D;average_price))+</span><br><span class="line">    geom_line(color&#x3D;&quot;#5C374C&quot;)+</span><br><span class="line">    theme_economist()+</span><br><span class="line">    theme(plot.title &#x3D; element_text(hjust &#x3D; 0.5),plot.background &#x3D; element_rect(fill &#x3D; &quot;#D5D8DC&quot;))+</span><br><span class="line">    labs(title &#x3D; &quot;Conventional Avocados&quot;)+</span><br><span class="line">    geom_hline(yintercept &#x3D; max(conventional$average_price),linetype&#x3D;&quot;dashed&quot;,color &#x3D; &quot;red&quot;)+</span><br><span class="line">    geom_hline(yintercept &#x3D; min(conventional$average_price),linetype&#x3D;&quot;dashed&quot;,color &#x3D; &quot;blue&quot;)</span><br><span class="line"></span><br><span class="line">organic_monthly &lt;- organic %&gt;%</span><br><span class="line">    ggplot(aes(x&#x3D;date,y&#x3D;average_price))+</span><br><span class="line">    geom_line(color&#x3D;&quot;#58D68D&quot;)+</span><br><span class="line">    theme_economist()+</span><br><span class="line">    theme(plot.title &#x3D; element_text(hjust &#x3D; 0.5),plot.background &#x3D; element_rect(fill &#x3D; &quot;#D5D8DC&quot;))+</span><br><span class="line">    labs(title &#x3D; &quot;Organic Avocados&quot;)+</span><br><span class="line">    geom_hline(yintercept &#x3D; max(organic$average_price),linetype&#x3D;&quot;dashed&quot;,color &#x3D; &quot;red&quot;)+</span><br><span class="line">    geom_hline(yintercept &#x3D; min(organic$average_price),linetype&#x3D;&quot;dashed&quot;,color &#x3D; &quot;blue&quot;)</span><br><span class="line"></span><br><span class="line">## create a volume chart</span><br><span class="line">conventional_volume &lt;- conventional %&gt;%</span><br><span class="line">    ggplot(aes(x&#x3D;date,y&#x3D;total_volume))+</span><br><span class="line">    geom_bar(stat &#x3D; &#39;identity&#39;,fill&#x3D;&quot;#7FB3D5&quot;,color&#x3D;&quot;black&quot;)+</span><br><span class="line">    theme_economist()+</span><br><span class="line">    theme(plot.title &#x3D; element_text(hjust &#x3D; 0.5),plot.background &#x3D; element_rect(fill &#x3D; &quot;#D5D8DC&quot;))+</span><br><span class="line">    geom_smooth(method &#x3D; &quot;loess&quot;,color&#x3D;&quot;red&quot;)</span><br><span class="line"></span><br><span class="line">organic_volume &lt;- organic %&gt;%</span><br><span class="line">    ggplot(aes(x&#x3D;date,y&#x3D;total_volume))+</span><br><span class="line">    geom_bar(stat &#x3D; &#39;identity&#39;,fill&#x3D;&#39;#58D68D&#39;,color&#x3D;&quot;black&quot;)+</span><br><span class="line">    theme_economist()+</span><br><span class="line">    theme(plot.title &#x3D; element_text(hjust &#x3D; 0.5),plot.background &#x3D; element_rect(fill &#x3D; &quot;#D5D8DC&quot;))+</span><br><span class="line">    geom_smooth(method &#x3D; &quot;loess&quot;,color &#x3D;&quot;red&quot;)</span><br><span class="line"></span><br><span class="line">plot_grid(conventional_monthly,organic_monthly,conventional_volume,organic_volume,nrow &#x3D; 2,ncol &#x3D; 2,labels &#x3D; c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;))</span><br><span class="line"></span><br><span class="line">#plot_grid(conventional_monthly,conventional_volume,nrow &#x3D; 2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://www.kaggleusercontent.com/kf/49257785/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..tZfBZbcjjkqxj4RmQEJoyw.A2C4X-_8qnKSkIVucnomHICENKUmjTPXUgT1XqFjrKp8Vwfpyt-nJXhYhke8yzjlDiL_FT1jvmw9GnBDuF_MeWzQUxn59txsXKEQ5TNxy2sZy6GYoHD4yeJb96CHu7tEvhTmc3Pf1nseFJUmMjAKCktdygT-uHDSuVQKAPNnGUpFNmn5fx_6OsujxRZO9TdXdWiqH1GGat_EqgxwZW-_o856VcxtNXe5WenELwGdFrP_fHsmRkvvcjtEmEXzl2_blPw6GMAdMmDD7WsNhKVd4zJoOVKvPx1awtn9623tYAqFfDJ3qzKqLfFAhNRAMsqQ-zfIos2ndjfK3YTRnRTKn_TX9TkPBnbni_OR6teuY24PW_hMZpFYW332SdtN2sCHyHke6Oaqgs6uq8l6jX14yyIMEhgiWkl5Usn36w1aLLtP4YQZIjO1m7B9FsVSOFeULVuVmThf6LyKuvsoG3N568gEqTKblQvj-_R0hUqGhc6W-3DZyE0hBoqgO__LBxbYYZ51DdMf-2bv2Hj99gNwuS-EVb1GFv__UrplkB63GGn-ftucHSQBkxSDoKlB0F99ZiGNWL4cmii009OqgG_mJQI9W-iqgB6sW3k3T3RdVL3QpdG-D3XEhYgfQTcMV6P23-9iQKNgOcTD1CWwWPVjsJfgu_NZHVIoUScEs9mdivE.1eUtwBW-D1O5QgrEgVmjhw/__results___files/__results___29_1.png"></p>
<ul>
<li>In order to find the seasonal patterns, I used the average price and volume in each month to conduct data analysis</li>
<li>Figure A,B stands for the average price in each month (monthly)</li>
<li>Blue dashed line stands for the minimum value while the red dashed value stands for the max value</li>
<li>The most expensive conventional avocados in one month could 1.8 $, the cheapest month of conventional avocados can be 0.82 $</li>
<li>The most expensive organic avocados in one month could 2.1 $, the cheapest month of conventional avocados can be 1.21 $</li>
<li>Figure C,D stands for the volume in each month, the red line stands for the trend</li>
<li>The love of American people are consistent, the sold volume continues to grow (conventional avocado)</li>
<li>Regarding with the organic avocados, in 2019-2020, possibly owing to the decline of economic situation and the covid 19 pandemic after 2020</li>
<li>The volume patterns could follow some seasonal patterns and need further analysis</li>
</ul>
<h3 id="Patterns-among-the-years-in-each-month-Autoplot-library"><a href="#Patterns-among-the-years-in-each-month-Autoplot-library" class="headerlink" title="Patterns among the years in each month (Autoplot library)"></a>Patterns among the years in each month (Autoplot library)</h3><figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Process the data into year and month format</span></span><br><span class="line">library(forecast)</span><br><span class="line">seasonal_df &lt;- read.csv(<span class="string">&quot;/Users/yuxuan/Desktop/INT303-Avocado-prediction/avocado-updated-2020.csv&quot;</span>)</span><br><span class="line">seasonal_df$month_year &lt;- format(as.Date(seasonal_df$date),<span class="string">&quot;%Y-%m&quot;</span>)</span><br><span class="line">seasonal_df$month &lt;- format(as.Date(seasonal_df$date),<span class="string">&quot;%m&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Change the month from a Date format into a numerical foramt, then convert to the three letter format</span></span><br><span class="line"></span><br><span class="line">seasonal_df$monthabb &lt;- sapply(seasonal_df$month, <span class="keyword">function</span> (x) <span class="built_in">month.abb</span>[<span class="built_in">as.numeric</span>(x)])</span><br><span class="line">seasonal_df$monthabb &lt;- factor(seasonal_df$monthabb,levels=<span class="built_in">month.abb</span>)</span><br><span class="line">seasonal_df$monthabb &lt;- factor(seasonal_df$monthabb)</span><br></pre></td></tr></table></figure>

<p>​                              </p>
<h3 id="Seasonal-patterns-analysis"><a href="#Seasonal-patterns-analysis" class="headerlink" title="Seasonal patterns analysis"></a>Seasonal patterns analysis</h3><p>Avocado price (both organic and conventional) distribution plot from 2015 to 2020</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ggplot(seasonal_df,aes(x&#x3D;average_price,fill&#x3D;as.factor(year)))+</span><br><span class="line">geom_density(alpha&#x3D;0.5)+</span><br><span class="line">theme_economist()+</span><br><span class="line">facet_wrap(~year)+</span><br><span class="line">theme(plot.title &#x3D; element_text(hjust &#x3D; 0.5),plot.background &#x3D; element_rect(fill&#x3D;&quot;#D5D8DC&quot;))+</span><br><span class="line">guides(fill&#x3D;FALSE)+</span><br><span class="line">labs(title &#x3D; &quot;Distribution of Prices by year&quot;,x&#x3D;&#39;Average Price&#39;,y&#x3D;&#39;Density&#39;)+</span><br><span class="line">scale_fill_manual(values &#x3D; c(&quot;#DA4511&quot;, &quot;#FFBD00&quot;, &quot;#6A953F&quot;, &quot;#9A6233&quot;, &quot;#D3AE7C&quot;, &quot;#307CA1&quot;))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://www.kaggleusercontent.com/kf/49257785/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Z0nhYvsbYiSXOwi0a3e_Vg.wiLxMBjmfH5zWgzbsyZmfX2LJ0G-TVaPvleMT_PnWnotXv9dMjLJOtv8HFfJB7VIKvaAiOuXOOnNoTfWMLNEsEmQ1mmHeVxFySzLDhVmEuNmhxbLPABCkDVdAAYu7lR3qLD4xuVLxb3CB1zqFhx00gKpCQ0KuMvuDogyMx8hZRnqRMpmN1BUdn9El9QgP0b39UCV6mFT_PHsMspROzMRXXWyLHa2OZk4LsOrllmxRmB2Z9QPNQumEVqNYhwLRXXaANlyENe5DUxBwdfHmK5317bJ12u_rA7mhEhbeV-0eqtoXoB-qJq2HVUfK1W8iEs6QQbGRR7HmPZ2HQO9EKmlZgJKoqLfa7YVA2yznP33QAvi-BHRStxgPRM36xNkRbGb8tTSf0KpaEqFZ2efZ15GqoD1LWYpCRxpZuuvV7B7Z24dQDZ-QfqKubUyzTDhrvSM3G0TBgWBt5fTNyP63r-FAuGXScA7BGlTIIyRQ23XG0w6Mg8X-ce0e2YbLK92ZgH9XIpuWrwdHEoNTCGhjWqrlJPSqkwLeceQhVxUlmEpVGQjC84iUebEnoPU7sNGzis1_haznEpo_OluFxMsvGGa6rpfmRKYHFo4U59pFOhZdUyAHICe_GdGbI73ue-mbyOKSs34XNlRW8m6ooJwM1Z2UaGKQqW1ZOyByXQKf3_Wobs.rI46PCTrAXYR5Yqoaj5Dlg/__results___files/__results___35_0.png" alt="img"></p>
<ul>
<li>The distribution plot of each year from 2015 to 2020</li>
<li>2017 is the year when avocado price functions like the normal distribution plot</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">## Set the figure size</span></span><br><span class="line"></span><br><span class="line">options(repr.plot.width=<span class="number">10</span>,repr.plot.height=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Analyze the price by month</span></span><br><span class="line"></span><br><span class="line">conv_price &lt;- seasonal_df %&gt;% select(type,year,monthabb,average_price) %&gt;% filter(type==<span class="string">&quot;conventional&quot;</span>) %&gt;% group_by(year,monthabb) %&gt;% summarise(avg=mean(average_price))</span><br><span class="line"></span><br><span class="line">org_price &lt;- seasonal_df %&gt;% select(type,year,monthabb,average_price) %&gt;% filter(type==<span class="string">&quot;organic&quot;</span>) %&gt;% group_by(year,monthabb) %&gt;% summarise(avg=mean(average_price))</span><br><span class="line"></span><br><span class="line">conv_price &lt;- ts(conv_price$avg,start = <span class="number">2015</span>,frequency = <span class="number">12</span>)</span><br><span class="line">org_price &lt;- ts(org_price$avg,start = <span class="number">2015</span>,frequency = <span class="number">12</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Analyze the volume by month</span></span><br><span class="line"></span><br><span class="line">conv_volume &lt;- seasonal_df %&gt;% select(type,year,monthabb,total_volume) %&gt;% filter(type==<span class="string">&quot;conventional&quot;</span>) %&gt;% group_by(year,monthabb) %&gt;% summarise(avg=mean(total_volume))</span><br><span class="line"></span><br><span class="line">org_volume &lt;- seasonal_df %&gt;% select(type,year,monthabb,total_volume) %&gt;% filter(type==<span class="string">&quot;organic&quot;</span>) %&gt;% group_by(year,monthabb) %&gt;% summarise(avg=mean(total_volume))</span><br><span class="line"></span><br><span class="line">conv_volume &lt;- ts(conv_volume$avg,start = <span class="number">2015</span>,frequency = <span class="number">12</span>)</span><br><span class="line">org_volume &lt;- ts(org_volume$avg,start = <span class="number">2015</span>,frequency = <span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">byyearplot_price_conv &lt;- ggseasonplot(conv_price,year.labels = <span class="literal">TRUE</span>,year.labels.left = <span class="literal">TRUE</span>)+</span><br><span class="line">theme_economist()+</span><br><span class="line">theme(plot.title = element_text(hjust = <span class="number">0.5</span>),plot.background = element_rect(fill=<span class="string">&quot;#D5D8DC&quot;</span>))+</span><br><span class="line">labs(title = <span class="string">&quot;Average conventional Avocados price \n by year for each month&quot;</span>, y=<span class="string">&quot;Average Price&quot;</span>)+</span><br><span class="line">scale_fill_manual(values = <span class="built_in">c</span>(<span class="string">&quot;#922B21&quot;</span>, <span class="string">&quot;#EE865D&quot;</span>, <span class="string">&quot;#DDCD5E&quot;</span>, <span class="string">&quot;#59BEC4&quot;</span>, <span class="string">&quot;#048B9F&quot;</span>, <span class="string">&quot;#114676&quot;</span>))</span><br><span class="line"></span><br><span class="line">byyearplot_price_org &lt;- ggseasonplot(org_price,year.labels = <span class="literal">TRUE</span>,year.labels.left = <span class="literal">TRUE</span>)+</span><br><span class="line">theme_economist()+</span><br><span class="line">theme(plot.title = element_text(hjust = <span class="number">0.5</span>),plot.background = element_rect(fill=<span class="string">&quot;#D5D8DC&quot;</span>))+</span><br><span class="line">labs(title = <span class="string">&quot;Average organic Avocados price \n by year for each month&quot;</span>, y=<span class="string">&quot;Average Price&quot;</span>)+</span><br><span class="line">scale_fill_manual(values = <span class="built_in">c</span>(<span class="string">&quot;#922B21&quot;</span>, <span class="string">&quot;#EE865D&quot;</span>, <span class="string">&quot;#DDCD5E&quot;</span>, <span class="string">&quot;#59BEC4&quot;</span>, <span class="string">&quot;#048B9F&quot;</span>, <span class="string">&quot;#114676&quot;</span>))</span><br><span class="line"></span><br><span class="line">byyearplot_volume_conv &lt;- ggseasonplot(conv_volume,year.labels = <span class="literal">TRUE</span>,year.labels.left = <span class="literal">TRUE</span>)+</span><br><span class="line">theme_economist()+</span><br><span class="line">theme(plot.title = element_text(hjust = <span class="number">0.5</span>),plot.background = element_rect(fill=<span class="string">&quot;#D5D8DC&quot;</span>))+</span><br><span class="line">labs(title = <span class="string">&quot;Average conventional Avocados volume \n by year for each month&quot;</span>, y=<span class="string">&quot;Average volume&quot;</span>)+</span><br><span class="line">scale_fill_manual(values = <span class="built_in">c</span>(<span class="string">&quot;#922B21&quot;</span>, <span class="string">&quot;#EE865D&quot;</span>, <span class="string">&quot;#DDCD5E&quot;</span>, <span class="string">&quot;#59BEC4&quot;</span>, <span class="string">&quot;#048B9F&quot;</span>, <span class="string">&quot;#114676&quot;</span>))</span><br><span class="line"></span><br><span class="line">byyearplot_volume_org &lt;- ggseasonplot(org_volume,year.labels = <span class="literal">TRUE</span>,year.labels.left = <span class="literal">TRUE</span>)+</span><br><span class="line">theme_economist()+</span><br><span class="line">theme(plot.title = element_text(hjust = <span class="number">0.5</span>),plot.background = element_rect(fill=<span class="string">&quot;#D5D8DC&quot;</span>))+</span><br><span class="line">labs(title = <span class="string">&quot;Average organic Avocados volume by year \n for each month&quot;</span>, y=<span class="string">&quot;Average volume&quot;</span>)+</span><br><span class="line">scale_fill_manual(values = <span class="built_in">c</span>(<span class="string">&quot;#922B21&quot;</span>, <span class="string">&quot;#EE865D&quot;</span>, <span class="string">&quot;#DDCD5E&quot;</span>, <span class="string">&quot;#59BEC4&quot;</span>, <span class="string">&quot;#048B9F&quot;</span>, <span class="string">&quot;#114676&quot;</span>))</span><br><span class="line"></span><br><span class="line">plot_grid(byyearplot_price_conv,byyearplot_price_org,byyearplot_volume_conv,byyearplot_volume_org,nrow = <span class="number">2</span>,ncol = <span class="number">2</span>,labels = <span class="built_in">c</span>(<span class="string">&quot;A&quot;</span>,<span class="string">&quot;B&quot;</span>,<span class="string">&quot;C&quot;</span>,<span class="string">&quot;D&quot;</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://www.kaggleusercontent.com/kf/49257785/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Z0nhYvsbYiSXOwi0a3e_Vg.wiLxMBjmfH5zWgzbsyZmfX2LJ0G-TVaPvleMT_PnWnotXv9dMjLJOtv8HFfJB7VIKvaAiOuXOOnNoTfWMLNEsEmQ1mmHeVxFySzLDhVmEuNmhxbLPABCkDVdAAYu7lR3qLD4xuVLxb3CB1zqFhx00gKpCQ0KuMvuDogyMx8hZRnqRMpmN1BUdn9El9QgP0b39UCV6mFT_PHsMspROzMRXXWyLHa2OZk4LsOrllmxRmB2Z9QPNQumEVqNYhwLRXXaANlyENe5DUxBwdfHmK5317bJ12u_rA7mhEhbeV-0eqtoXoB-qJq2HVUfK1W8iEs6QQbGRR7HmPZ2HQO9EKmlZgJKoqLfa7YVA2yznP33QAvi-BHRStxgPRM36xNkRbGb8tTSf0KpaEqFZ2efZ15GqoD1LWYpCRxpZuuvV7B7Z24dQDZ-QfqKubUyzTDhrvSM3G0TBgWBt5fTNyP63r-FAuGXScA7BGlTIIyRQ23XG0w6Mg8X-ce0e2YbLK92ZgH9XIpuWrwdHEoNTCGhjWqrlJPSqkwLeceQhVxUlmEpVGQjC84iUebEnoPU7sNGzis1_haznEpo_OluFxMsvGGa6rpfmRKYHFo4U59pFOhZdUyAHICe_GdGbI73ue-mbyOKSs34XNlRW8m6ooJwM1Z2UaGKQqW1ZOyByXQKf3_Wobs.rI46PCTrAXYR5Yqoaj5Dlg/__results___files/__results___37_1.png" alt="img"></p>
<h3 id="Seasonality-patterns"><a href="#Seasonality-patterns" class="headerlink" title="Seasonality patterns"></a>Seasonality patterns</h3><h4 id="Monthly-analysis"><a href="#Monthly-analysis" class="headerlink" title="Monthly analysis"></a>Monthly analysis</h4><figure class="highlight plain"><figcaption><span>seaonal patterns, fig.align </span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">options(repr.plot.width&#x3D;10,repr.plot.height&#x3D;8)</span><br><span class="line">conv_patterns &lt;- seasonal_df %&gt;% select(monthabb,average_price,type) %&gt;% filter(type&#x3D;&#x3D;&quot;conventional&quot;) %&gt;% group_by(monthabb) %&gt;% summarise(avg&#x3D;mean(average_price)) %&gt;%</span><br><span class="line">  ggplot(aes(x&#x3D;monthabb, y&#x3D;avg))+</span><br><span class="line">  geom_point(color&#x3D;&quot;#F35D5D&quot;,aes(size&#x3D;avg))+</span><br><span class="line">  geom_line(group&#x3D;0)+</span><br><span class="line">  theme_economist()+</span><br><span class="line">  theme(legend.position &#x3D; &quot;none&quot;,plot.title &#x3D; element_text(hjust &#x3D; 0.5),plot.background &#x3D; element_rect(fill&#x3D;&quot;#D5D8DC&quot;))+</span><br><span class="line">  labs(title &#x3D; &quot;Conventional Avocados&quot;,x&#x3D;&quot;Month&quot;,y&#x3D;&quot;Average Price&quot;)</span><br><span class="line"></span><br><span class="line">organic_patterns &lt;- seasonal_df %&gt;% select(monthabb,average_price,type) %&gt;% filter(type&#x3D;&#x3D;&quot;organic&quot;) %&gt;% group_by(monthabb) %&gt;% summarise(avg&#x3D;mean(average_price)) %&gt;%</span><br><span class="line">ggplot(aes(x&#x3D;monthabb,y&#x3D;avg))+</span><br><span class="line">  geom_point(color&#x3D;&quot;#F35D5D&quot;,aes(size&#x3D;avg))+</span><br><span class="line">  geom_line(group&#x3D;0)+</span><br><span class="line">  theme_economist()+</span><br><span class="line">  theme(legend.position &#x3D; &quot;none&quot;,plot.title &#x3D; element_text(hjust &#x3D; 0.5),plot.background &#x3D; element_rect(fill&#x3D;&quot;#D5D8DC&quot;))+</span><br><span class="line">  labs(title &#x3D; &quot;Organic Avocados&quot;,x&#x3D;&quot;Month&quot;,y&#x3D;&quot;Average Price&quot;)</span><br><span class="line"></span><br><span class="line">whole_patterns &lt;- seasonal_df %&gt;% select(monthabb,average_price,type) %&gt;% group_by(monthabb) %&gt;% summarise(avg&#x3D;mean(average_price)) %&gt;%</span><br><span class="line">  ggplot(aes(x&#x3D;monthabb,y&#x3D;avg))+</span><br><span class="line">  geom_point(color&#x3D;&quot;#F35D5D&quot;,aes(size&#x3D;avg))+</span><br><span class="line">  geom_line(group&#x3D;0)+</span><br><span class="line">  theme_economist()+</span><br><span class="line">  theme(legend.position &#x3D; &quot;none&quot;,plot.title &#x3D; element_text(hjust &#x3D; 0.5),plot.background &#x3D; element_rect(fill&#x3D;&quot;#D5D8DC&quot;))+</span><br><span class="line">  labs(title &#x3D; &quot;All Avocados&quot;,x&#x3D;&quot;Month&quot;,y&#x3D;&quot;Average Price&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plot_grid(conv_patterns,organic_patterns,whole_patterns,nrow &#x3D; 3)</span><br><span class="line">#conv_patterns</span><br></pre></td></tr></table></figure>
<p><img src="https://www.kaggleusercontent.com/kf/49257785/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Z0nhYvsbYiSXOwi0a3e_Vg.wiLxMBjmfH5zWgzbsyZmfX2LJ0G-TVaPvleMT_PnWnotXv9dMjLJOtv8HFfJB7VIKvaAiOuXOOnNoTfWMLNEsEmQ1mmHeVxFySzLDhVmEuNmhxbLPABCkDVdAAYu7lR3qLD4xuVLxb3CB1zqFhx00gKpCQ0KuMvuDogyMx8hZRnqRMpmN1BUdn9El9QgP0b39UCV6mFT_PHsMspROzMRXXWyLHa2OZk4LsOrllmxRmB2Z9QPNQumEVqNYhwLRXXaANlyENe5DUxBwdfHmK5317bJ12u_rA7mhEhbeV-0eqtoXoB-qJq2HVUfK1W8iEs6QQbGRR7HmPZ2HQO9EKmlZgJKoqLfa7YVA2yznP33QAvi-BHRStxgPRM36xNkRbGb8tTSf0KpaEqFZ2efZ15GqoD1LWYpCRxpZuuvV7B7Z24dQDZ-QfqKubUyzTDhrvSM3G0TBgWBt5fTNyP63r-FAuGXScA7BGlTIIyRQ23XG0w6Mg8X-ce0e2YbLK92ZgH9XIpuWrwdHEoNTCGhjWqrlJPSqkwLeceQhVxUlmEpVGQjC84iUebEnoPU7sNGzis1_haznEpo_OluFxMsvGGa6rpfmRKYHFo4U59pFOhZdUyAHICe_GdGbI73ue-mbyOKSs34XNlRW8m6ooJwM1Z2UaGKQqW1ZOyByXQKf3_Wobs.rI46PCTrAXYR5Yqoaj5Dlg/__results___files/__results___39_1.png" alt="img"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conv_patterns_vol &lt;- seasonal_df %&gt;% select(monthabb,total_volume,type) %&gt;% filter(type&#x3D;&#x3D;&quot;conventional&quot;) %&gt;% group_by(monthabb) %&gt;% summarise(avg&#x3D;mean(total_volume)) %&gt;%</span><br><span class="line">  ggplot(aes(x&#x3D;monthabb, y&#x3D;avg))+</span><br><span class="line">  geom_point(color&#x3D;&quot;#F35D5D&quot;,aes(size&#x3D;avg))+</span><br><span class="line">  geom_line(group&#x3D;0)+</span><br><span class="line">  theme_economist()+</span><br><span class="line">  theme(legend.position &#x3D; &quot;none&quot;,plot.title &#x3D; element_text(hjust &#x3D; 0.5),plot.background &#x3D; element_rect(fill&#x3D;&quot;#D5D8DC&quot;))+</span><br><span class="line">  labs(title &#x3D; &quot;Conventional Avocados volume&quot;,x&#x3D;&quot;Month&quot;,y&#x3D;&quot;Average Price&quot;)</span><br><span class="line"></span><br><span class="line">organic_patterns_vol &lt;- seasonal_df %&gt;% select(monthabb,total_volume,type) %&gt;% filter(type&#x3D;&#x3D;&quot;organic&quot;) %&gt;% group_by(monthabb) %&gt;% summarise(avg&#x3D;mean(total_volume)) %&gt;%</span><br><span class="line">ggplot(aes(x&#x3D;monthabb,y&#x3D;avg))+</span><br><span class="line">  geom_point(color&#x3D;&quot;#F35D5D&quot;,aes(size&#x3D;avg))+</span><br><span class="line">  geom_line(group&#x3D;0)+</span><br><span class="line">  theme_economist()+</span><br><span class="line">  theme(legend.position &#x3D; &quot;none&quot;,plot.title &#x3D; element_text(hjust &#x3D; 0.5),plot.background &#x3D; element_rect(fill&#x3D;&quot;#D5D8DC&quot;))+</span><br><span class="line">  labs(title &#x3D; &quot;Organic Avocados volume&quot;,x&#x3D;&quot;Month&quot;,y&#x3D;&quot;Average Price&quot;)</span><br><span class="line"></span><br><span class="line">whole_patterns_vol &lt;- seasonal_df %&gt;% select(monthabb,total_volume,type) %&gt;% group_by(monthabb) %&gt;% summarise(avg&#x3D;mean(total_volume)) %&gt;%</span><br><span class="line">  ggplot(aes(x&#x3D;monthabb,y&#x3D;avg))+</span><br><span class="line">  geom_point(color&#x3D;&quot;#F35D5D&quot;,aes(size&#x3D;avg))+</span><br><span class="line">  geom_line(group&#x3D;0)+</span><br><span class="line">  theme_economist()+</span><br><span class="line">  theme(legend.position &#x3D; &quot;none&quot;,plot.title &#x3D; element_text(hjust &#x3D; 0.5),plot.background &#x3D; element_rect(fill&#x3D;&quot;#D5D8DC&quot;))+</span><br><span class="line">  labs(title &#x3D; &quot;All Avocados volume&quot;,x&#x3D;&quot;Month&quot;,y&#x3D;&quot;Average Price&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plot_grid(conv_patterns_vol,organic_patterns_vol,whole_patterns_vol,nrow &#x3D; 3)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://www.kaggleusercontent.com/kf/49257785/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Z0nhYvsbYiSXOwi0a3e_Vg.wiLxMBjmfH5zWgzbsyZmfX2LJ0G-TVaPvleMT_PnWnotXv9dMjLJOtv8HFfJB7VIKvaAiOuXOOnNoTfWMLNEsEmQ1mmHeVxFySzLDhVmEuNmhxbLPABCkDVdAAYu7lR3qLD4xuVLxb3CB1zqFhx00gKpCQ0KuMvuDogyMx8hZRnqRMpmN1BUdn9El9QgP0b39UCV6mFT_PHsMspROzMRXXWyLHa2OZk4LsOrllmxRmB2Z9QPNQumEVqNYhwLRXXaANlyENe5DUxBwdfHmK5317bJ12u_rA7mhEhbeV-0eqtoXoB-qJq2HVUfK1W8iEs6QQbGRR7HmPZ2HQO9EKmlZgJKoqLfa7YVA2yznP33QAvi-BHRStxgPRM36xNkRbGb8tTSf0KpaEqFZ2efZ15GqoD1LWYpCRxpZuuvV7B7Z24dQDZ-QfqKubUyzTDhrvSM3G0TBgWBt5fTNyP63r-FAuGXScA7BGlTIIyRQ23XG0w6Mg8X-ce0e2YbLK92ZgH9XIpuWrwdHEoNTCGhjWqrlJPSqkwLeceQhVxUlmEpVGQjC84iUebEnoPU7sNGzis1_haznEpo_OluFxMsvGGa6rpfmRKYHFo4U59pFOhZdUyAHICe_GdGbI73ue-mbyOKSs34XNlRW8m6ooJwM1Z2UaGKQqW1ZOyByXQKf3_Wobs.rI46PCTrAXYR5Yqoaj5Dlg/__results___files/__results___40_1.png" alt="img"></p>
<ul>
<li>Overall, the avocado price could reach the peak of the year during Sep and Oct, and Feb could be the lowest price</li>
<li>Regarding with the volume avocado sold, American people like to buy avocado at Feb and May, but not at Nov</li>
<li>Based on the research, avocado tend to ripe at Aug and Sep, plus the time in transportation and packaging, it is unavoidable that it could have some delay</li>
<li>From the plot we could see that the trend in Aug are positive</li>
<li>We could see that the sold volume and price demonstrated some negative correlation at some time, which obeys our common sense, people like buying staffs when their price are low</li>
<li>Meanwhile, some positive correlation could be discovered between price and volumes, the hypothesis is that the newly harvest avocados are definitely in high quality and the previous stored avocados could be consumed significantly, which means there is a gap between demand and output</li>
<li>The volumes are increasing with the time which possibly because of the widely broadcasting.</li>
</ul>
<h4 id="Seasonal-patterns"><a href="#Seasonal-patterns" class="headerlink" title="Seasonal patterns"></a>Seasonal patterns</h4><figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="code"><pre><span class="line">options(repr.plot.width&#x3D;10,repr.plot.height&#x3D;8)</span><br><span class="line"></span><br><span class="line">## seperate the month into four seasons</span><br><span class="line">seasonal_df$season &lt;- ifelse(seasonal_df$month %in% c(&quot;03&quot;,&quot;04&quot;,&quot;05&quot;),&quot;Spring&quot;,</span><br><span class="line">ifelse(seasonal_df$month %in% c(&quot;06&quot;,&quot;07&quot;,&quot;08&quot;),&quot;Summer&quot;,</span><br><span class="line">ifelse(seasonal_df$month %in% c(&quot;09&quot;,&quot;10&quot;,&quot;11&quot;),&quot;Autumn&quot;,&quot;Winter&quot;)))</span><br><span class="line"></span><br><span class="line">## Prepare to analyze the results</span><br><span class="line">seasonality_plot_conventional_price &lt;- seasonal_df %&gt;% select(season,year,average_price,type) %&gt;% filter(type &#x3D;&#x3D;&quot;conventional&quot;) %&gt;% group_by(season,year) %&gt;% summarise(avg&#x3D;mean(average_price)) %&gt;%</span><br><span class="line">  ggplot(aes(x&#x3D;season,y&#x3D;avg,color&#x3D;season))+</span><br><span class="line">  geom_segment(aes(x&#x3D;season,xend&#x3D;season,y&#x3D;0,yend&#x3D;avg),show.legend &#x3D; FALSE)+</span><br><span class="line">  coord_flip()+</span><br><span class="line">  facet_wrap(~as.factor(year))+</span><br><span class="line">  theme_minimal()+</span><br><span class="line">  theme(plot.title &#x3D; element_text(hjust &#x3D; 0.5),plot.background &#x3D; element_rect(fill&#x3D;&quot;#F4F6F7&quot;))+</span><br><span class="line">  labs(title &#x3D; &quot;Conventional Avocados average price by Season&quot;,x&#x3D;&quot;&quot;,y&#x3D;&quot;Average price&quot;)+</span><br><span class="line">  geom_text(aes(x&#x3D;season,y&#x3D;0.01,label&#x3D;paste0(&quot;$ &quot;,round(avg,2))),hjust&#x3D;-0.5,vjust&#x3D;-0.5,size&#x3D;4,color&#x3D;&quot;black&quot;,fontface&#x3D;&#39;italic&#39;,angle&#x3D;360)</span><br><span class="line"></span><br><span class="line">seasonality_plot_conventional_volume &lt;- seasonal_df %&gt;% select(season,year,total_volume,type) %&gt;% filter(type&#x3D;&#x3D;&quot;conventional&quot;) %&gt;% group_by(season,year) %&gt;% summarise(avg&#x3D;round(mean(total_volume&#x2F;1000000),2)) %&gt;%</span><br><span class="line">  ggplot(aes(x&#x3D;season,y&#x3D;avg,color&#x3D;season))+</span><br><span class="line">  geom_segment(aes(x&#x3D;season,xend&#x3D;season,y&#x3D;0,yend&#x3D;avg),show.legend &#x3D; FALSE)+</span><br><span class="line">  coord_flip()+</span><br><span class="line">  facet_wrap(~as.factor(year))+</span><br><span class="line">  theme_minimal()+</span><br><span class="line">  theme(plot.title &#x3D; element_text(hjust &#x3D; 0.5),plot.background &#x3D; element_rect(fill&#x3D;&quot;#F4F6F7&quot;))+</span><br><span class="line">  labs(title &#x3D; &quot;Conventional Avocados total volume by Season&quot;,x&#x3D;&quot;&quot;,y&#x3D;&quot;Average volume&quot;)+</span><br><span class="line">  geom_text(aes(x&#x3D;season,y&#x3D;0.01,label&#x3D;paste0(avg,&quot; m&quot;)),hjust&#x3D;-0.5,vjust&#x3D;-0.5,size&#x3D;4,color&#x3D;&quot;black&quot;,fontface&#x3D;&#39;italic&#39;,angle&#x3D;360)</span><br><span class="line"></span><br><span class="line">#plot_grid(seasonality_plot_conventional_price,seasonality_plot_conventional_volume,nrow &#x3D; 2)</span><br><span class="line"></span><br><span class="line">seasonality_plot_organic_price &lt;- seasonal_df %&gt;% select(season,year,average_price,type) %&gt;% filter(type &#x3D;&#x3D;&quot;organic&quot;) %&gt;% group_by(season,year) %&gt;% summarise(avg&#x3D;mean(average_price)) %&gt;%</span><br><span class="line">  ggplot(aes(x&#x3D;season,y&#x3D;avg,color&#x3D;season))+</span><br><span class="line">  geom_segment(aes(x&#x3D;season,xend&#x3D;season,y&#x3D;0,yend&#x3D;avg),show.legend &#x3D; FALSE)+</span><br><span class="line">  coord_flip()+</span><br><span class="line">  facet_wrap(~as.factor(year))+</span><br><span class="line">  theme_minimal()+</span><br><span class="line">  theme(plot.title &#x3D; element_text(hjust &#x3D; 0.5),plot.background &#x3D; element_rect(fill&#x3D;&quot;#F4F6F7&quot;))+</span><br><span class="line">  labs(title &#x3D; &quot;Organic Avocados average price by Season&quot;,x&#x3D;&quot;&quot;,y&#x3D;&quot;Average price&quot;)+</span><br><span class="line">  geom_text(aes(x&#x3D;season,y&#x3D;0.01,label&#x3D;paste0(&quot;$ &quot;,round(avg,2))),hjust&#x3D;-0.5,vjust&#x3D;-0.5,size&#x3D;4,color&#x3D;&quot;black&quot;,fontface&#x3D;&#39;italic&#39;,angle&#x3D;360)</span><br><span class="line"></span><br><span class="line">seasonality_plot_organic_volume &lt;- seasonal_df %&gt;% select(season,year,total_volume,type) %&gt;% filter(type&#x3D;&#x3D;&quot;organic&quot;) %&gt;% group_by(season,year) %&gt;% summarise(avg&#x3D;round(mean(total_volume&#x2F;1000000),2)) %&gt;%</span><br><span class="line">  ggplot(aes(x&#x3D;season,y&#x3D;avg,color&#x3D;season))+</span><br><span class="line">  geom_segment(aes(x&#x3D;season,xend&#x3D;season,y&#x3D;0,yend&#x3D;avg),show.legend &#x3D; FALSE)+</span><br><span class="line">  coord_flip()+</span><br><span class="line">  facet_wrap(~as.factor(year))+</span><br><span class="line">  theme_minimal()+</span><br><span class="line">  theme(plot.title &#x3D; element_text(hjust &#x3D; 0.5),plot.background &#x3D; element_rect(fill&#x3D;&quot;#F4F6F7&quot;))+</span><br><span class="line">  labs(title &#x3D; &quot;Organic Avocados total volume by Season&quot;,x&#x3D;&quot;&quot;,y&#x3D;&quot;Average volume&quot;)+</span><br><span class="line">  geom_text(aes(x&#x3D;season,y&#x3D;0.01,label&#x3D;paste0(avg,&quot; m&quot;)),hjust&#x3D;-0.5,vjust&#x3D;-0.5,size&#x3D;4,color&#x3D;&quot;black&quot;,fontface&#x3D;&#39;italic&#39;,angle&#x3D;360)</span><br><span class="line"></span><br><span class="line">plot_grid(seasonality_plot_conventional_price,seasonality_plot_organic_price,seasonality_plot_conventional_volume,seasonality_plot_organic_volume,nrow &#x3D; 2,ncol &#x3D; 2,labels &#x3D; c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://www.kaggleusercontent.com/kf/49257785/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Z0nhYvsbYiSXOwi0a3e_Vg.wiLxMBjmfH5zWgzbsyZmfX2LJ0G-TVaPvleMT_PnWnotXv9dMjLJOtv8HFfJB7VIKvaAiOuXOOnNoTfWMLNEsEmQ1mmHeVxFySzLDhVmEuNmhxbLPABCkDVdAAYu7lR3qLD4xuVLxb3CB1zqFhx00gKpCQ0KuMvuDogyMx8hZRnqRMpmN1BUdn9El9QgP0b39UCV6mFT_PHsMspROzMRXXWyLHa2OZk4LsOrllmxRmB2Z9QPNQumEVqNYhwLRXXaANlyENe5DUxBwdfHmK5317bJ12u_rA7mhEhbeV-0eqtoXoB-qJq2HVUfK1W8iEs6QQbGRR7HmPZ2HQO9EKmlZgJKoqLfa7YVA2yznP33QAvi-BHRStxgPRM36xNkRbGb8tTSf0KpaEqFZ2efZ15GqoD1LWYpCRxpZuuvV7B7Z24dQDZ-QfqKubUyzTDhrvSM3G0TBgWBt5fTNyP63r-FAuGXScA7BGlTIIyRQ23XG0w6Mg8X-ce0e2YbLK92ZgH9XIpuWrwdHEoNTCGhjWqrlJPSqkwLeceQhVxUlmEpVGQjC84iUebEnoPU7sNGzis1_haznEpo_OluFxMsvGGa6rpfmRKYHFo4U59pFOhZdUyAHICe_GdGbI73ue-mbyOKSs34XNlRW8m6ooJwM1Z2UaGKQqW1ZOyByXQKf3_Wobs.rI46PCTrAXYR5Yqoaj5Dlg/__results___files/__results___43_1.png" alt="img"></p>
<ul>
<li>Spring (3-5), Summer (6-8), Autumn (9-11), Winter (12-2)</li>
<li>Figure A, B stands for the average price of avocados for either conventional or organic</li>
<li>Figure C, D stands for the volume sold for either conventional or organic, the unit is million (m)</li>
<li>In summary:<ul>
<li>it is relative cheaper to avocado in Spring or Winter</li>
<li>avocado most sold in Spring and Summer since it is close to the next round of ripen in avocado and the market are in great supply of avocados</li>
</ul>
</li>
</ul>
<h3 id="Find-the-city-where-avocado’s-price-is-lowest"><a href="#Find-the-city-where-avocado’s-price-is-lowest" class="headerlink" title="Find the city where avocado’s price is lowest"></a>Find the city where avocado’s price is lowest</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(forcats)</span><br><span class="line">geo_conv_price &lt;- seasonal_df %&gt;% select(geography,average_price,type) %&gt;% filter(type&#x3D;&#x3D;&quot;conventional&quot;) %&gt;% group_by(geography)%&gt;%  summarise(avg&#x3D;round(mean(average_price),2)) %&gt;% arrange(avg) %&gt;% slice(1:6)  %&gt;% mutate(geography&#x3D;fct_reorder(geography,desc(avg))) %&gt;%</span><br><span class="line">  ggplot(aes(x&#x3D;geography,y&#x3D;avg))+</span><br><span class="line">  geom_bar(stat &#x3D; &quot;identity&quot;,position &#x3D; &quot;dodge&quot;,alpha&#x3D;.6,width &#x3D;.4,show.legend &#x3D; FALSE)+</span><br><span class="line">  coord_flip()+</span><br><span class="line">  geom_text(aes(label&#x3D;avg),vjust&#x3D;1.5,color&#x3D;&#39;black&#39;,size&#x3D;5)+</span><br><span class="line">  theme_minimal()+</span><br><span class="line">  theme(title &#x3D; element_text(hjust &#x3D; 0.6),plot.background &#x3D; element_rect(fill&#x3D;&quot;#F4F6F7&quot;))+</span><br><span class="line">  labs(title &#x3D; &quot;Conventional Avocados average price by geography&quot;,x&#x3D;&quot;Geography place&quot;,y&#x3D;&quot;Average Price&quot;)</span><br><span class="line"></span><br><span class="line">geo_org_price &lt;- seasonal_df %&gt;% select(geography,average_price,type) %&gt;% filter(type&#x3D;&#x3D;&quot;organic&quot;) %&gt;% group_by(geography)%&gt;%  summarise(avg&#x3D;round(mean(average_price),2)) %&gt;% arrange(avg) %&gt;% slice(1:6) %&gt;% mutate(geography&#x3D;fct_reorder(geography,desc(avg))) %&gt;%</span><br><span class="line">  ggplot(aes(x&#x3D;geography,y&#x3D;avg))+</span><br><span class="line">  geom_bar(stat &#x3D; &quot;identity&quot;,position &#x3D; &quot;dodge&quot;,alpha&#x3D;.6,width &#x3D;.4,show.legend &#x3D; FALSE)+</span><br><span class="line">  coord_flip()+</span><br><span class="line">  geom_text(aes(label&#x3D;avg),vjust&#x3D;1.5,color&#x3D;&#39;black&#39;,size&#x3D;5)+</span><br><span class="line">  theme_minimal()+</span><br><span class="line">  theme(title &#x3D; element_text(hjust &#x3D; 0.6),plot.background &#x3D; element_rect(fill&#x3D;&quot;#F4F6F7&quot;))+</span><br><span class="line">  labs(title &#x3D; &quot;Organic Avocados average price by geography&quot;,x&#x3D;&quot;Geography place&quot;,y&#x3D;&quot;Average Price&quot;)</span><br><span class="line"></span><br><span class="line">plot_grid(geo_conv_price,geo_org_price,nrow &#x3D; 2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://www.kaggleusercontent.com/kf/49257785/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Z0nhYvsbYiSXOwi0a3e_Vg.wiLxMBjmfH5zWgzbsyZmfX2LJ0G-TVaPvleMT_PnWnotXv9dMjLJOtv8HFfJB7VIKvaAiOuXOOnNoTfWMLNEsEmQ1mmHeVxFySzLDhVmEuNmhxbLPABCkDVdAAYu7lR3qLD4xuVLxb3CB1zqFhx00gKpCQ0KuMvuDogyMx8hZRnqRMpmN1BUdn9El9QgP0b39UCV6mFT_PHsMspROzMRXXWyLHa2OZk4LsOrllmxRmB2Z9QPNQumEVqNYhwLRXXaANlyENe5DUxBwdfHmK5317bJ12u_rA7mhEhbeV-0eqtoXoB-qJq2HVUfK1W8iEs6QQbGRR7HmPZ2HQO9EKmlZgJKoqLfa7YVA2yznP33QAvi-BHRStxgPRM36xNkRbGb8tTSf0KpaEqFZ2efZ15GqoD1LWYpCRxpZuuvV7B7Z24dQDZ-QfqKubUyzTDhrvSM3G0TBgWBt5fTNyP63r-FAuGXScA7BGlTIIyRQ23XG0w6Mg8X-ce0e2YbLK92ZgH9XIpuWrwdHEoNTCGhjWqrlJPSqkwLeceQhVxUlmEpVGQjC84iUebEnoPU7sNGzis1_haznEpo_OluFxMsvGGa6rpfmRKYHFo4U59pFOhZdUyAHICe_GdGbI73ue-mbyOKSs34XNlRW8m6ooJwM1Z2UaGKQqW1ZOyByXQKf3_Wobs.rI46PCTrAXYR5Yqoaj5Dlg/__results___files/__results___47_1.png" alt="img"></p>
<ul>
<li>Use the calculation to return the six cities or regions whether the average price for avocados is lowest</li>
<li>Based on the research from map, Top 5 is close to the Mexico, where the avocado originated</li>
<li>These places are possibly the region to supply avocado</li>
</ul>
<h3 id="The-future-price-prediction"><a href="#The-future-price-prediction" class="headerlink" title="The future price prediction"></a>The future price prediction</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(prophet)</span><br><span class="line">library(cowplot)</span><br><span class="line">library(gridExtra)</span><br><span class="line">library(ggplot2)</span><br><span class="line">library(ggpubr)</span><br><span class="line">library(devtools)</span><br><span class="line">## Select and filter the data and fit the model</span><br><span class="line">price_predict &lt;- df %&gt;% select(date,average_price) %&gt;% group_by(date) %&gt;% summarise(avg&#x3D;mean(average_price))</span><br><span class="line">colnames(price_predict) &lt;- c(&#39;ds&#39;,&#39;y&#39;)</span><br><span class="line">model &lt;- prophet(price_predict,daily.seasonality &#x3D; TRUE)</span><br><span class="line"></span><br><span class="line">## make prediction or forecast the results</span><br><span class="line">future &lt;- make_future_dataframe(model,periods &#x3D; 365)</span><br><span class="line">forecast &lt;- predict(model,future)</span><br><span class="line">plot(model,forecast)+</span><br><span class="line">theme_minimal()+</span><br><span class="line">  theme(plot.title &#x3D; element_text(hjust &#x3D; 0.5),plot.background &#x3D; element_rect(fill&#x3D;&quot;#F4F6F7&quot;))+</span><br><span class="line">  labs(title &#x3D; &quot;Avocados price prediction in next year&quot;,x&#x3D;&quot;year&quot;,y&#x3D;&quot;Average Price&quot;)</span><br><span class="line"></span><br><span class="line">trend &lt;- prophet_plot_components(model,forecast)</span><br><span class="line"></span><br><span class="line">year_trend &lt;- trend[1]</span><br><span class="line">month_trend &lt;- trend[3]</span><br><span class="line">week_trend &lt;- trend[2]</span><br><span class="line">time_trend &lt;- trend[4]</span><br><span class="line"></span><br><span class="line">overall_trend &lt;- c(year_trend,month_trend,week_trend,time_trend)</span><br><span class="line">ggarrange(plotlist &#x3D; overall_trend, labels &#x3D; c(&#39;A&#39;, &#39;B&#39;,&#39;C&#39;,&#39;D&#39;))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://www.kaggleusercontent.com/kf/49257785/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Z0nhYvsbYiSXOwi0a3e_Vg.wiLxMBjmfH5zWgzbsyZmfX2LJ0G-TVaPvleMT_PnWnotXv9dMjLJOtv8HFfJB7VIKvaAiOuXOOnNoTfWMLNEsEmQ1mmHeVxFySzLDhVmEuNmhxbLPABCkDVdAAYu7lR3qLD4xuVLxb3CB1zqFhx00gKpCQ0KuMvuDogyMx8hZRnqRMpmN1BUdn9El9QgP0b39UCV6mFT_PHsMspROzMRXXWyLHa2OZk4LsOrllmxRmB2Z9QPNQumEVqNYhwLRXXaANlyENe5DUxBwdfHmK5317bJ12u_rA7mhEhbeV-0eqtoXoB-qJq2HVUfK1W8iEs6QQbGRR7HmPZ2HQO9EKmlZgJKoqLfa7YVA2yznP33QAvi-BHRStxgPRM36xNkRbGb8tTSf0KpaEqFZ2efZ15GqoD1LWYpCRxpZuuvV7B7Z24dQDZ-QfqKubUyzTDhrvSM3G0TBgWBt5fTNyP63r-FAuGXScA7BGlTIIyRQ23XG0w6Mg8X-ce0e2YbLK92ZgH9XIpuWrwdHEoNTCGhjWqrlJPSqkwLeceQhVxUlmEpVGQjC84iUebEnoPU7sNGzis1_haznEpo_OluFxMsvGGa6rpfmRKYHFo4U59pFOhZdUyAHICe_GdGbI73ue-mbyOKSs34XNlRW8m6ooJwM1Z2UaGKQqW1ZOyByXQKf3_Wobs.rI46PCTrAXYR5Yqoaj5Dlg/__results___files/__results___52_0.png" alt="img"></p>
<ul>
<li>Figure A is analyze based on year: Since 2015, the average price of avocados continues to grow and 2017 was a crazy year, they grow in a speed higher than 1.3; After 2018, the average price tend to be stable, but still has a rate about 1.28; Regarding to the future prediction, the range is about [1.20-1.34]. But considering the pandemic in North America, the possible rate could be 1.20</li>
<li>Figure B is the analysis based on month: we could see that during May, the price tend to grow in a positive way and this trend continues to grow to Oct; After Oct, the price starts to decrease, which follows the previous analysis</li>
<li>Figure C is the analysis based on week. The average price follow the rule that ordinary family shopping habits, they prefer to buy things during weekends, so the price during weekends could be higher in other days</li>
</ul>
<hr>
<h3 id="Other-platforms"><a href="#Other-platforms" class="headerlink" title="Other platforms:"></a>Other platforms:</h3><p><strong>kaggle:</strong> <a class="link"   href="https://www.kaggle.com/yuxuanwu17/eda-of-avocado-price-from-2015-to-2020" >https://www.kaggle.com/yuxuanwu17/eda-of-avocado-price-from-2015-to-2020<i class="fas fa-external-link-alt"></i></a></p>
<p><strong>zhihu (Chinese version):</strong> <a class="link"   href="https://zhuanlan.zhihu.com/p/332699778" >https://zhuanlan.zhihu.com/p/332699778<i class="fas fa-external-link-alt"></i></a></p>
<p><strong>Github:</strong> <a class="link"   href="https://github.com/yuxuanwu17/INT303-Avocado-prediction" >https://github.com/yuxuanwu17/INT303-Avocado-prediction<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <tags>
        <tag>kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop based big data analysis</title>
    <url>/2021/01/25/Hadoop/</url>
    <content><![CDATA[<p>Hadoop comprises three part:</p>
<ol>
<li>Main function, including some basic configuration of job</li>
<li>Mapper class, inherited from and override the Map function</li>
<li>Reduce class, inherited from Reduce function and override it</li>
</ol>
<h3 id="1-Calculate-the-bigram-frequency-and-return-the-Top-10"><a href="#1-Calculate-the-bigram-frequency-and-return-the-Top-10" class="headerlink" title="1. Calculate the bigram frequency and return the Top 10"></a>1. Calculate the bigram frequency and return the Top 10</h3><hr>
<h4 id="data-cleaning-part-single-world"><a href="#data-cleaning-part-single-world" class="headerlink" title="data cleaning part (single world)"></a>data cleaning part (single world)</h4><ul>
<li>Use regular expression to extract the useful information</li>
<li>For the neat of code, I separate the word cleaning part into one method: deleteNotion</li>
<li>The idea is to loop all the words and feed into the deleteNotion function to clean the data</li>
<li>Since the word fed into the program is a txt file, we need first calculate each word by .split(“\s+”), and called String [] single_word</li>
<li>The regular expression is designed to match all the punctuation and numbers except for English words </li>
<li>Note that \W didn’t include the number, so we have to make a union of numbers and other punctuation in \W, that is [\\w[0-9]]</li>
<li>Create an ArrayList to store the processed data.</li>
</ul>
<h4 id="bigram-extraction"><a href="#bigram-extraction" class="headerlink" title="bigram extraction"></a>bigram extraction</h4><ul>
<li>loop through the single_word [] list</li>
<li>Note that: since we are calculating the bigram, the length should be length(single_word)-1</li>
<li>We also need to ensure that the first and last were not empty</li>
<li>Then add the data to the newly created Arraylist</li>
</ul>
<hr>
<h3 id="2-Return-the-line-containing-the-word-‘torture’"><a href="#2-Return-the-line-containing-the-word-‘torture’" class="headerlink" title="2. Return the line containing the word ‘torture’"></a>2. Return the line containing the word ‘torture’</h3><ul>
<li>Use regular expression to separate line and store it in to the list</li>
<li>The regular expression was written as {\\n{1,}}, detecting the “\n” and store it as one line </li>
<li>Create an arraylist results to each line </li>
<li>Loop through the arraylist and use .contain function to find the line contain “torture”</li>
<li>Note that, the mapper variable and reduce variable should be switched to Text, the attribute problem should take it seriously</li>
</ul>
<hr>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><h4 id="bigram-frequency"><a href="#bigram-frequency" class="headerlink" title="bigram frequency"></a>bigram frequency</h4><p><img src="/fig/wordcount.png" alt="wordcount"></p>
<h4 id="lines-containing-“torture”"><a href="#lines-containing-“torture”" class="headerlink" title="lines containing “torture”"></a>lines containing “torture”</h4><p><img src="/fig/Shuffled_Maps.png" alt="img"></p>
<hr>
<h2 id="Code-resources"><a href="#Code-resources" class="headerlink" title="Code resources"></a>Code resources</h2><p>Github: <a class="link"   href="https://github.com/yuxuanwu17/Hadoop_cw2" >https://github.com/yuxuanwu17/Hadoop_cw2<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>MATLAB based neural network for image classification</title>
    <url>/2021/01/25/matlab/</url>
    <content><![CDATA[<h2 id="Project-overview"><a href="#Project-overview" class="headerlink" title="Project overview :"></a>Project overview :</h2><p>This task includes five algorithms to classify the characters segmented from the license plates of automobiles using MATLAB</p>
<ul>
<li>MLP (multi-layer perceptron)</li>
<li>CNN (convolutional neural network)</li>
<li>LVQ (learning vector quantization)</li>
<li>RBF1 with k-means clustering</li>
<li>RBF2 with SOM (self-organizing map)</li>
</ul>
<hr>
<h2 id="Files-running-order"><a href="#Files-running-order" class="headerlink" title="Files running order"></a>Files running order</h2><ol>
<li>run the figure_preprocessing.m and create a new file folder ass2_processede_data</li>
<li>run data_partition.m split the dataset in 8:2 version and returned the X_train, X_test, y_train, y_test and save as train_test_data.mat</li>
<li>run ass2_CNN.m (this script did not use the processed data, but read the figure directly from the original file)</li>
<li>run ass2_mlp.m </li>
<li>run ass2_lvq.m</li>
<li>run ass2_rbf_kmean.m</li>
<li>run ass2_rbf_som.m</li>
<li>run ass2_confusion_matrix_summary.m</li>
</ol>
<hr>
<h2 id="Components-Scripts-inside-the-project"><a href="#Components-Scripts-inside-the-project" class="headerlink" title="Components/Scripts inside the project"></a>Components/Scripts inside the project</h2><h2 id="The-script-part"><a href="#The-script-part" class="headerlink" title="The script part"></a>The script part</h2><h3 id="figure-preprocessing-m"><a href="#figure-preprocessing-m" class="headerlink" title="figure_preprocessing.m"></a>figure_preprocessing.m</h3><p>This is the script to extract the ass2data and rewrite to a new file folder ass2_processed_data. </p>
<pre><code>- read both jpeg and jpg files in all folders ass2_data
- rewrite into another folders ass2_processed_data with uniform style (jpeg) and uniform naming styles (label+number) e.g. (A1,B10,C99)</code></pre>
<h3 id="data-partition-m"><a href="#data-partition-m" class="headerlink" title="data_partition.m"></a>data_partition.m</h3><p>This is the script splitted the processed data into training and testing dataset in 8:2 ratio. It adjusted the size and columns or rows for fitting the designed model’s network.</p>
<pre><code>- read images from the processed folder ass2_processed_data. 
- split the dataset into 8:2
- save the splitted samples and parameters as train_test_data.mat</code></pre>
<h3 id="ass2-CNN-m"><a href="#ass2-CNN-m" class="headerlink" title="ass2_CNN.m"></a>ass2_CNN.m</h3><p>This is the script to conduct the CNN.</p>
<pre><code>- use imageDataset to store and train the model
- use splitEachLabel to split the training and testing datasets
- do not use the train_test_data as the input since CNN has standard samples in Matlab documents
- imageInputLayer(48,24,1)
- Layer 1:
    - kernel size: 3; filters number: 8; padding: same
    - batchNormalizationLayer
    - Relu layer
    - Maxpooling with a pooling size equals [2 2] and Stride equals [2 2]
- Layer 2:
    - kernel size: 3; filters number: 16; padding: same    
    - batchNormalizationLayer
    - Relu layer
    - Maxpooling with a pooling size equals [2 2] and Stride equals [2 2]
- Layer 3:
    - kernel size: 3; filters number: 32; padding: same    
    - batchNormalizationLayer
    - Relu layer
    - Maxpooling with a pooling size equals [2 2] and Stride equals [2 2]
- Layer 4:
    - fully connected layer with neurons equals 24
    - softmaxLayer
- calculate the training and testing accuracy
- return and save the confusion matrix as C_CNN.mat</code></pre>
<h3 id="ass2-mlp-m"><a href="#ass2-mlp-m" class="headerlink" title="ass2_mlp.m"></a>ass2_mlp.m</h3><p>This is the script to conduct the MLP</p>
<pre><code>- load the dataset obtained previously train_test_data.mat
- lr_rate = 0.2;
- momentum = 0.4;
- epochs = 1000;
- 3 layers with 50, 100, 100 
- Hyperbolic tangent sigmoid transfer function (tansig)
- gradient descent to update the weights
- calculate the training and testing accuracy
- return and save the confusion matrix as C_mlp.mat</code></pre>
<h3 id="ass2-lvq-m"><a href="#ass2-lvq-m" class="headerlink" title="ass2_lvq.m"></a>ass2_lvq.m</h3><pre><code>- load the dataset obtained previously train_test_data.mat
- set the cluster into 360 due to its performance, you could also set cluster into 24 for computational convenience
- lvqnet(24) could achieve ideal performance, but the performance could not be comparable with the cluster set into 360
- save the lvqnet(360) as lvq_360.mat, uncomment the line if you wish to see the performance
- calculate the training and testing accuracy
- return and save the confusion matrix C_lvq.mat</code></pre>
<h3 id="ass2-rbf-kmean-m"><a href="#ass2-rbf-kmean-m" class="headerlink" title="ass2_rbf_kmean.m"></a>ass2_rbf_kmean.m</h3><pre><code>- load the dataset obtained previously train_test_data.mat
- use kmeans to return the center of each cluster, the number of cluster is determined as 360
- use assembled function RBF_training_kmeans to calculate the W (weights), sigma (the variance of the RBF kernel) and the coordinate of each cluster's center 
- use the previous returned parameter to return the training prediction by assembled function: RBF_predict
- calculate the training and testing accuracy
- return and save the confusion matrix C_rbf_kmeans.mat</code></pre>
<h3 id="ass2-rbf-som-m"><a href="#ass2-rbf-som-m" class="headerlink" title="ass2_rbf_som.m"></a>ass2_rbf_som.m</h3><pre><code>- load the dataset obtained previously train_test_data.mat
- define the SOM network, the dimension is set to 18*20 for convenience, keep it uniform to the previous number of clusters: 360
- coverSteps = 10 %% Number of training steps for initial covering of the input space (default = 100)
- initNeighbor = 80 %% Initial neighborhood size (default = 3)
- topologyFcn = 'hextop' %% Layer topology function (default = 'hextop')
- distanceFcn = 'dist' %%  Neuron distance function (default = 'linkdist')
- once finished the training of SOM network, use assembled function RBF_training_som.m to calculate the weights W, sigma (the variance of the RBF kernel) and center of SOM clusters, which is the IW{1,1}.
- use the previous returned parameter to return the training prediction by assembled function: RBF_predict
- return and save the confusion matrix C_rbf_som.mat</code></pre>
<h3 id="ass2-confusion-matrix-summary-m"><a href="#ass2-confusion-matrix-summary-m" class="headerlink" title="ass2_confusion_matrix_summary.m"></a>ass2_confusion_matrix_summary.m</h3><pre><code>- load the previous saved mat file
- reshow the confusion matrix
- compare the performance returned by different neural networks
- compare the accuracy returned by different neural networks  </code></pre>
<hr>
<h2 id="The-function-part-assembled-function-for-reproduction"><a href="#The-function-part-assembled-function-for-reproduction" class="headerlink" title="The function part (assembled function for reproduction)"></a>The function part (assembled function for reproduction)</h2><h3 id="getimdata-m"><a href="#getimdata-m" class="headerlink" title="getimdata.m"></a>getimdata.m</h3><pre><code>- Input: the file path, in this case the processed file with renamed figure and same format jpeg (ass2_processed_data) 
- Output: Transormed the numerical data into the one-hot encoding vector format (one-hot format)</code></pre>
<h3 id="getimdata2-m"><a href="#getimdata2-m" class="headerlink" title="getimdata2.m"></a>getimdata2.m</h3><pre><code>- Input: the file path, in this case the processed file with renamed figure and same format jpeg (ass2_processed_data) 
    - normalize the data into range 0-1
    - assign the numerical labels to each character
    - combine all the processed plot into a dataset 2400 * 1152
    - 2400 stands for the number of samples; 1152 stands for the features in one figure
- Output: The normalized figure data (0-1), with data and corresponding labels. Labels are in numerical format (1,2,3...24) (without one-hot)
- This function method is selected in the whole project</code></pre>
<h3 id="RBF-training-kmeans-m"><a href="#RBF-training-kmeans-m" class="headerlink" title="RBF_training_kmeans.m"></a>RBF_training_kmeans.m</h3><pre><code>- Input: data, labels, number of clusters to be determined by kmeans
- sigma is determined by the mean Euclidean distance between two clusters
- k weight matrix is calculated by the radbas(distance of samples between clusters' centers/2*sigma^2)
- W weights is calculated by the pesudo inverse of (k'*k)*k'*labels</code></pre>
<h3 id="RBF-training-som-m"><a href="#RBF-training-som-m" class="headerlink" title="RBF_training_som.m"></a>RBF_training_som.m</h3><pre><code>- Input: data, labels and net
- net is pre-trained by som networks
- the cluster center is returned by the first layer of som network, which is denoted by net.IW{1,1}
- sigma is determined by the mean Euclidean distance between two clusters
- k weight matrix is calculated by the radbas(distance of samples between clusters' centers/2*sigma^2)
- W weights is calculated by the pesudo inverse of (k'*k)*k'*labels</code></pre>
<h3 id="RBF-predict-m"><a href="#RBF-predict-m" class="headerlink" title="RBF_predict.m"></a>RBF_predict.m</h3><pre><code>- Input: data, W, sigma, C trained previously from either RBF_training_kmeans or RBF_training_som
- Output: vectors of the final prediction
- data could be either training data or testing data</code></pre>
<h3 id="getcls-m"><a href="#getcls-m" class="headerlink" title="getcls.m"></a>getcls.m</h3><pre><code>- Input: vecs - matrix of column vectors (returned from the RBF_predict.m)
- Output: cls - matrix where the largest element in each column in vectors is set to 1 and the rest to 0   Ex: vecs = [2 4; 1 5], gives c = [1 0; 0 1]
- This function is used to return the most likely label in multi-variable classification, especially after the one-hot encoding method</code></pre>
<h3 id="rate-m"><a href="#rate-m" class="headerlink" title="rate.m"></a>rate.m</h3><pre><code>- Input: matrix of class vectors
- Computes the percentage of equal columns in t1 and t2, can be used to compute the rate of correct classified patterns in a pattern recognition application
- Output: number of matching vectors</code></pre>
<hr>
<h2 id="Saved-parameters"><a href="#Saved-parameters" class="headerlink" title="Saved parameters"></a>Saved parameters</h2><h3 id="train-test-data-mat"><a href="#train-test-data-mat" class="headerlink" title="train_test_data.mat"></a>train_test_data.mat</h3><pre><code>- the value obtained from the data_partition.m
- X_train, X_test, y_train, y_value</code></pre>
<h3 id="som-net-data-mat"><a href="#som-net-data-mat" class="headerlink" title="som_net_data.mat"></a>som_net_data.mat</h3><pre><code>- the network trained by som (it takes long time, for computation convenience)</code></pre>
<h3 id="lvq-360-mat"><a href="#lvq-360-mat" class="headerlink" title="lvq_360.mat"></a>lvq_360.mat</h3><pre><code>- lvq with cluster set to 360</code></pre>
<h3 id="lvq-24-mat"><a href="#lvq-24-mat" class="headerlink" title="lvq_24.mat"></a>lvq_24.mat</h3><pre><code>- lvq with cluster set to 24 </code></pre>
<h3 id="kmeans-plot-mat"><a href="#kmeans-plot-mat" class="headerlink" title="kmeans_plot.mat"></a>kmeans_plot.mat</h3><pre><code>- calculate silhouette value to find the suitable cluster k, but the results are not satisfied
- return the plot of each epoch</code></pre>
<h3 id="Confusion-matrix-summary"><a href="#Confusion-matrix-summary" class="headerlink" title="Confusion_matrix summary:"></a>Confusion_matrix summary:</h3><pre><code>- CNN_confusion.mat
- C_rbf_som.mat
- C_rbf_kmeans.mat
- C_mlp.mat
- C_lvq.mat
- C_CNN.mat</code></pre>
<hr>
<p>Assessment2</p>
<p>You could download the whole file with script and report in 1716309_Yuxuan_Wu.zip from my own repository</p>
<p><a class="link" href="https://github.com/yuxuanwu17/INT301_Assessment2">https://github.com/yuxuanwu17/INT301_Assessment2<i class="fas fa-external-link-alt"></i></a></p>
]]></content>
      <tags>
        <tag>Matlab</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux htop 详解</title>
    <url>/2021/01/25/Linux_htop/</url>
    <content><![CDATA[<p>之前在学校实验室工作的时候，接触的第一个命令就是htop。之前就是觉得这个花花绿绿的界面很有趣（牛逼），如下图，常常在女朋友面前故作牛逼的打开（<em>实际就看看我的程序在不在跑，虚伪的不行🤪</em>）</p>
<p><img src="https://pic1.zhimg.com/v2-317c74bdfc0734f88813c5fe532d3220_b.png" alt="img"></p>
<p>为了以后装逼更有资本，我觉得得仔细弄清楚这些值到底是干什么的</p>
<p>本篇文章参考</p>
<p>Understanding Output of htop Command - Linux Togetherlinuxtogether.org</p>
<h3 id="htop是top的升级版-允许用户监视系统上运行的进程及其完整的命令行"><a href="#htop是top的升级版-允许用户监视系统上运行的进程及其完整的命令行" class="headerlink" title="htop是top的升级版,允许用户监视系统上运行的进程及其完整的命令行"></a>htop是top的升级版,允许用户监视系统上运行的进程及其完整的命令行</h3><ol>
<li>系统不会自带，若是Ubuntu，sudo apt, 若是Centos， yum 可以安装。根据不过系统来进行安装</li>
<li>支持用户交互，可以通过鼠标来kill进程而不用通过输入其PID，支持用鼠标上下拖动，且不同的颜色代表不同的意思。</li>
<li>允许用户根据CPU，内存和时间间隔对进程进行排序</li>
</ol>
<h3 id="htop-的安装"><a href="#htop-的安装" class="headerlink" title="htop 的安装"></a>htop 的安装</h3><p>Ubuntu 安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install htop</span><br></pre></td></tr></table></figure>
<p>CentOS 安装</p>
<p><a class="link"   href="http://www.vue5.com/centos/23046.htmlwww.vue5.com" >http://www.vue5.com/centos/23046.htmlwww.vue5.com<i class="fas fa-external-link-alt"></i></a></p>
<p><strong>基本的概念</strong></p>
<p>tasks/process 进程， thread 线程相关的知识可以从下面的链接学习</p>
<p><a class="link"   href="https://www.zhihu.com/question/307100151/answer/894486042" >如何理解：程序、进程、线程、并发、并行、高并发？<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   href="https://zhuanlan.zhihu.com/p/125716637" >宇宙之一粟：为什么校招面试中“线程与进程的区别”老是被问到?我该如何回答？<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   href="https://www.geeksforgeeks.org/difference-between-process-and-thread/" >Difference between Process and Thread - GeeksforGeeks<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   href="https://zhuanlan.zhihu.com/p/82746153" >什么是虚拟内存(Virtual Memory)?<i class="fas fa-external-link-alt"></i></a></p>
<p><img src="https://pic2.zhimg.com/v2-f876d2e42e8d5e1110f22f7198fcc591_b.png" alt="img"></p>
<h3 id="红色边框"><a href="#红色边框" class="headerlink" title="红色边框"></a>红色边框</h3><p><em>数字1，2，3，4分别代表CPU处理器/核，上图是一个四核的处理器</em></p>
<h3 id="灰色边框（progress-bar）"><a href="#灰色边框（progress-bar）" class="headerlink" title="灰色边框（progress bar）"></a>灰色边框（progress bar）</h3><p><em>每一个CPU的总用量情况，注意这条上面会有不同的颜色：</em></p>
<ol>
<li>蓝色：显示低优先级(low priority)进程使用的CPU百分比。 </li>
<li>绿色：显示用于普通用户(user)拥有的进程的CPU百分比。 </li>
<li>红色：显示系统进程(kernel threads)使用的CPU百分比。</li>
<li>橙色：显示IRQ时间使用的CPU百分比。</li>
<li>洋红色(Magenta)：显示Soft IRQ时间消耗的CPU百分比。</li>
<li>灰色：显示IO等待时间消耗的CPU百分比。</li>
<li>青色：显示窃取时间(Steal time)消耗的CPU百分比。</li>
</ol>
<h3 id="黄色边框"><a href="#黄色边框" class="headerlink" title="黄色边框"></a>黄色边框</h3><p><em>提供了内存（Memory）和交换（Swap）使用情况。 类似于CPU中的进度条，内存监视也包含具有多种颜色的进度条：</em></p>
<ol>
<li>绿色：显示内存页面占用的RAM百分比</li>
<li>蓝色：显示缓冲区页面占用的RAM百分比</li>
<li>橙色：显示缓存页面占用的RAM百分比</li>
</ol>
<p><img src="https://pic1.zhimg.com/v2-e9a5539a25c37421fc429424aab510b8_b.png" alt="img"></p>
<h3 id="蓝色边框"><a href="#蓝色边框" class="headerlink" title="蓝色边框"></a>蓝色边框</h3><ul>
<li><p><strong>第一行 (Tasks, thr, running)</strong></p>
</li>
<li><p>参考上面的屏幕截图，我们在计算机上运行的106个任务(tasks)被分解为113个线程(thread)，其中只有1个进程处于运行(running)状态。</p>
</li>
<li><p>任务(tasks)是打开的进程总数的代表，但并不是每个打开的进程都在不断消耗CPU。 每个进程都处于几种状态</p>
</li>
<li><p>R: Running：表示进程(process)正在使用CPU</p>
</li>
<li><p>S: Sleeping: 通常进程在大多数时间都处于睡眠状态，并以固定的时间间隔执行小检查，或者等待用户输入后再返回运行状态。</p>
</li>
<li><p>T/S: Traced/Stoped: 表示进程正在处于暂停的状态</p>
</li>
<li><p>Z:Zombie or defunct:已完成执行但在进程表中仍具有条目的进程。</p>
</li>
<li><p><strong>第二行  Load Average</strong></p>
</li>
<li><p>三个值是指系统在最后1分钟，最近5分钟和最后15分钟的平均负载 (0.21,0.19,0.15)</p>
</li>
<li><p><strong>第三行 Uptime</strong></p>
</li>
<li><p>表示这个系统一共运行了多长的时间，这里一共运行了78天</p>
</li>
</ul>
<h3 id="下半部分"><a href="#下半部分" class="headerlink" title="下半部分"></a>下半部分</h3><p><img src="https://pic2.zhimg.com/v2-dd7ca8f231cfa317243987fd4bf02189_b.png" alt="img"></p>
<ul>
<li>PID – 描述进程的ID号</li>
<li>USER – 描述进程的所有者（谁跑的）</li>
<li>PRI – 描述Linux内核查看的进程优先级</li>
<li>NI – 描述由用户或root重置的进程优先级</li>
<li>VIR – 它描述进程正在使用的虚拟内存 （virtual memory）</li>
<li>RES – 描述进程正在消耗的物理内存（physical memory）</li>
<li>SHR – 描述进程正在使用的共享内存（shared memory）</li>
<li>S – 描述流程的当前状态 (state)</li>
<li>CPU％ – 描述每个进程消耗的CPU百分比</li>
<li>MEM％ – 描述每个进程消耗的内存百分比</li>
<li>TIME+ – 显示自流程开始执行以来的时间</li>
<li>Command –它与每个进程并行显示完整的命令执行 (比如/usr/lib/R)</li>
</ul>
<hr>
<h3 id="一些快捷键"><a href="#一些快捷键" class="headerlink" title="一些快捷键"></a>一些快捷键</h3><p>u – 用于显示特定用户拥有的所有进程。</p>
<p>P –用于基于高CPU消耗对进程进行排序。</p>
<p>M –用于基于高内存消耗对进程进行排序。</p>
<p>T –用于根据时间段对过程进行排序。</p>
<p>h –用于打开帮助窗口并查看此处未提及的更多快捷方式。</p>
<p><strong>帮助： htop -h</strong></p>
<p><strong>更细致的解释：man htop</strong></p>
<p><img src="https://pic1.zhimg.com/v2-3aece676cc1a36e4ea439e75ce81b238_b.png" alt="img"></p>
]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>基于MATLAB的图片提取和分析</title>
    <url>/2021/01/25/MATLAB%E5%9B%BE%E7%89%87%E6%8F%90%E5%8F%96%E4%B8%8E%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>完成学校Matlab作业后的一些总结，数据可以从我的GitHub账户下载。作为一名Matlab的初学者，请多多海涵</p>
<p><a class="link" href="https://github.com/yuxuanwu17/INT301/blob/main/ass2data.zip">数据在此<i class="fas fa-external-link-alt"></i></a></p>
<p>数据是来自车牌号图片（冷知识：I,O天生是在车牌里不出现的）。一共24个文件夹，每个文件夹里有100张图片，所以一共有2400张图片。这次的项目是构建预测模型来进行图像识别。</p>
<p><img src="https://pic1.zhimg.com/v2-442cbdaf16d424739603ef3c267914dc_b.png" alt="img"></p>
<p>我们打开这些文件，查看图片的详细信息</p>
<p><img src="https://pic1.zhimg.com/v2-37aefc51ea4c68ec2b8dddee4939a708_b.png" alt="img"></p>
<p>我们发现这里面的图片格式不一致，且命名没有统一规律。</p>
<p>下文开始处理图片数据，并将每一张照片提取出来，用Matlab分析</p>
<h3 id="方法一：暴力修改文档法"><a href="#方法一：暴力修改文档法" class="headerlink" title="方法一：暴力修改文档法"></a>方法一：暴力修改文档法</h3><p>核心思路：将图片全部提取出来，然后统一格式，统一名称，保存在一个新的文件夹里</p>
<p>参考 <a class="link" href="https://blog.csdn.net/FX677588/article/details/53126961">matlab遍历文件夹下所有图片和遍历所有子文件夹下图片_无鞋童鞋的博客-CSDN博客_matlab遍历文件夹下子文件夹所有文件<i class="fas fa-external-link-alt"></i></a></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">imgDataPath = <span class="string">'../INT301_Assessment2/ass2data/'</span>;</span><br><span class="line">imgDataDir  = dir(imgDataPath);             <span class="comment">% 遍历该路径下的所有文件</span></span><br><span class="line">savepath =  <span class="string">'../INT301_Assessment2/ass2_processed_data/'</span>; <span class="comment">% 要改写的文件路径</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="built_in">length</span>(imgDataDir)</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">isequal</span>(imgDataDir(<span class="built_in">i</span>).name,<span class="string">'.'</span>)||... <span class="comment">% 去除系统自带的两个隐文件夹</span></span><br><span class="line">       <span class="built_in">isequal</span>(imgDataDir(<span class="built_in">i</span>).name,<span class="string">'..'</span>)||...</span><br><span class="line">       <span class="built_in">isequal</span>(imgDataDir(<span class="built_in">i</span>).name,<span class="string">'.DS_Store'</span>)||...</span><br><span class="line">       ~imgDataDir(<span class="built_in">i</span>).isdir)                <span class="comment">% 去除遍历中不是文件夹的</span></span><br><span class="line">           <span class="keyword">continue</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    imgDir_jpeg = dir([imgDataPath imgDataDir(<span class="built_in">i</span>).name <span class="string">'/*.jpeg'</span>]); <span class="comment">%读取jpeg格式的图片</span></span><br><span class="line">    imgDir_jpg = dir([imgDataPath imgDataDir(<span class="built_in">i</span>).name <span class="string">'/*.jpg'</span>]); <span class="comment">%读取jpg格式的图片</span></span><br><span class="line">    imgDir = [imgDir_jpeg',imgDir_jpg']'; <span class="comment">% 合并成子文件夹下的所有图片地址</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> =<span class="number">1</span>:<span class="built_in">length</span>(imgDir)                 <span class="comment">% 遍历所有文件的地址</span></span><br><span class="line">        img = imread([imgDataPath imgDataDir(<span class="built_in">i</span>).name <span class="string">'/'</span> imgDir(<span class="built_in">j</span>).name]); <span class="comment">%读取单个文件</span></span><br><span class="line">        imwrite(img, [savepath,imgDataDir(<span class="built_in">i</span>).name,num2str(<span class="built_in">j</span>),<span class="string">'.jpeg'</span>]) <span class="comment">%以字符+编号为命名</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>新建立的文件夹，检查一下有没有漏，可以直接看每个字母的最后一个是不是”字母100.jpeg”</p>
<p><img src="https://pic4.zhimg.com/v2-98d05aac73ddf39e77aa168050c19a97_b.png" alt="img"></p>
<h3 id="处理单个图片，以矩阵形式保存倒入的图片"><a href="#处理单个图片，以矩阵形式保存倒入的图片" class="headerlink" title="处理单个图片，以矩阵形式保存倒入的图片"></a>处理单个图片，以矩阵形式保存倒入的图片</h3><p>下图getimdata2这个函数输入的是重写后的新文件夹地址，输出的是训练集X和标签y （这里用数字代指标签）</p>
<p>不好意思有点懒得动脑子了，就直接写了24个if，大家可以随意优化</p>
<p>如果继续处理例如转换为onehot之类的可以直接用ind2vec来转换</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[ data,target ]</span> = <span class="title">getimdata2</span><span class="params">(path)</span></span></span><br><span class="line"><span class="comment">%GETIMDATA Summary of this function goes here</span></span><br><span class="line"><span class="comment">%   Detailed explanation goes here</span></span><br><span class="line">    files = dir([path <span class="string">'*.jpeg'</span>]);</span><br><span class="line">    data=[];</span><br><span class="line">    target=[];</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> file = files'</span><br><span class="line">        im = imread([path file.name]);</span><br><span class="line">        <span class="comment">% Normalize the data </span></span><br><span class="line">        im = double(im);</span><br><span class="line">        im = im/<span class="number">255</span>;</span><br><span class="line">        </span><br><span class="line">        data = [data im(:)];</span><br><span class="line">        <span class="keyword">if</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'A'</span>) <span class="comment">% 如果文件第一个字符等于‘A‘，那就用1代替a</span></span><br><span class="line">            target=[target [<span class="number">1</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'B'</span>)</span><br><span class="line">            target=[target [<span class="number">2</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'C'</span>)</span><br><span class="line">            target=[target [<span class="number">3</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'D'</span>)</span><br><span class="line">            target=[target [<span class="number">4</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'E'</span>)</span><br><span class="line">            target=[target [<span class="number">5</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'F'</span>)</span><br><span class="line">            target=[target [<span class="number">6</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'G'</span>)</span><br><span class="line">            target=[target [<span class="number">7</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'H'</span>)</span><br><span class="line">            target=[target [<span class="number">8</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'J'</span>)</span><br><span class="line">            target=[target [<span class="number">9</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'K'</span>)</span><br><span class="line">            target=[target [<span class="number">10</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'L'</span>)</span><br><span class="line">            target=[target [<span class="number">11</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'M'</span>)</span><br><span class="line">            target=[target [<span class="number">12</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'N'</span>)</span><br><span class="line">            target=[target [<span class="number">13</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'P'</span>)</span><br><span class="line">            target=[target [<span class="number">14</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'Q'</span>)</span><br><span class="line">            target=[target [<span class="number">15</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'R'</span>)</span><br><span class="line">            target=[target [<span class="number">16</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'S'</span>)</span><br><span class="line">            target=[target [<span class="number">17</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'T'</span>)</span><br><span class="line">            target=[target [<span class="number">18</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'U'</span>)</span><br><span class="line">            target=[target [<span class="number">19</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'V'</span>)</span><br><span class="line">            target=[target [<span class="number">20</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'W'</span>)</span><br><span class="line">            target=[target [<span class="number">21</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'X'</span>)</span><br><span class="line">            target=[target [<span class="number">22</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'Y'</span>)</span><br><span class="line">            target=[target [<span class="number">23</span>]];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'Z'</span>)</span><br><span class="line">            target=[target [<span class="number">24</span>]];</span><br><span class="line">        <span class="keyword">end</span>    </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>另外一个getimdata 函数和上面大体差不多，但是输出的是已经onehot encoding后的结果（e.g. A-&gt; (1,0,0,…0)）标签用一个1*24的向量来表示</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[ data,target ]</span> = <span class="title">getimdata</span><span class="params">(path)</span></span></span><br><span class="line"><span class="comment">%GETIMDATA Summary of this function goes here</span></span><br><span class="line"><span class="comment">%   Detailed explanation goes here</span></span><br><span class="line">    files = dir([path <span class="string">'*.jpeg'</span>]);</span><br><span class="line">    data=[];</span><br><span class="line">    target=[];</span><br><span class="line">    </span><br><span class="line"><span class="comment">%   Generated the cell and zero matrix to store</span></span><br><span class="line">    store = cell(<span class="number">1</span>,<span class="number">24</span>);</span><br><span class="line">    tst_zero = <span class="built_in">zeros</span>(<span class="number">1</span>,<span class="number">24</span>);</span><br><span class="line">    </span><br><span class="line"><span class="comment">%   Fill the zeros matrix </span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="number">24</span></span><br><span class="line">        copy = tst_zero;</span><br><span class="line">        copy(<span class="number">1</span>,<span class="built_in">i</span>) = <span class="number">1</span>;</span><br><span class="line">        store{<span class="built_in">i</span>} = copy;</span><br><span class="line">    <span class="keyword">end</span> </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> file = files'</span><br><span class="line">        im = imread([path file.name]);</span><br><span class="line">        <span class="comment">% Normalize the data </span></span><br><span class="line">        im = double(im);</span><br><span class="line">        im = im/<span class="number">255</span>;</span><br><span class="line">        data = [data im(:)];</span><br><span class="line">        test_zero = <span class="built_in">zeros</span>(<span class="number">1</span>,<span class="number">24</span>);</span><br><span class="line">        <span class="keyword">if</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'A'</span>)</span><br><span class="line">            target=[target [store{<span class="number">1</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'B'</span>)</span><br><span class="line">            target=[target [store{<span class="number">2</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'C'</span>)</span><br><span class="line">            target=[target [store{<span class="number">3</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'D'</span>)</span><br><span class="line">            target=[target [store{<span class="number">4</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'E'</span>)</span><br><span class="line">            target=[target [store{<span class="number">5</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'F'</span>)</span><br><span class="line">            target=[target [store{<span class="number">6</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'G'</span>)</span><br><span class="line">            target=[target [store{<span class="number">7</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'H'</span>)</span><br><span class="line">            target=[target [store{<span class="number">8</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'J'</span>)</span><br><span class="line">            target=[target [store{<span class="number">9</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'K'</span>)</span><br><span class="line">            target=[target [store{<span class="number">10</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'L'</span>)</span><br><span class="line">            target=[target [store{<span class="number">11</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'M'</span>)</span><br><span class="line">            target=[target [store{<span class="number">12</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'N'</span>)</span><br><span class="line">            target=[target [store{<span class="number">13</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'P'</span>)</span><br><span class="line">            target=[target [store{<span class="number">14</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'Q'</span>)</span><br><span class="line">            target=[target [store{<span class="number">15</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'R'</span>)</span><br><span class="line">            target=[target [store{<span class="number">16</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'S'</span>)</span><br><span class="line">            target=[target [store{<span class="number">17</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'T'</span>)</span><br><span class="line">            target=[target [store{<span class="number">18</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'U'</span>)</span><br><span class="line">            target=[target [store{<span class="number">19</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'V'</span>)</span><br><span class="line">            target=[target [store{<span class="number">20</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'W'</span>)</span><br><span class="line">            target=[target [store{<span class="number">21</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'X'</span>)</span><br><span class="line">            target=[target [store{<span class="number">22</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'Y'</span>)</span><br><span class="line">            target=[target [store{<span class="number">23</span>}]'];</span><br><span class="line">        <span class="keyword">elseif</span> strcmp(file.name(<span class="number">1</span>),<span class="string">'Z'</span>)</span><br><span class="line">            target=[target [store{<span class="number">24</span>}]'];</span><br><span class="line">        <span class="keyword">end</span>    </span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<h3 id="方法二：利用imageDatastore"><a href="#方法二：利用imageDatastore" class="headerlink" title="方法二：利用imageDatastore"></a>方法二：利用imageDatastore</h3><p>简便的方法永远是在你写完做完后发现的🙄</p>
<p>imageDatastore 可以直接读取你这个文件夹下的所有子文件及其图片</p>
<p>下面的代码是直接将数据集和标签都已经分好处理好了，可以直接喂到模型里面去的</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">%%</span></span><br><span class="line">clc;</span><br><span class="line">clear;</span><br><span class="line"></span><br><span class="line"><span class="comment">%% </span></span><br><span class="line">F = <span class="string">'../INT301_Assessment2/ass2data'</span>;</span><br><span class="line">imds = imageDatastore(F,<span class="string">'IncludeSubfolders'</span>,<span class="built_in">true</span>,<span class="string">'LabelSource'</span>,<span class="string">'foldernames'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line">labelCount = countEachLabel(imds);   <span class="comment">%统计imds中各标签值的图片数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% 2，8分割，8为training，2为testing</span></span><br><span class="line">numTrainFiles = <span class="number">80</span>;</span><br><span class="line">[imdsTrain, imdsTest] = splitEachLabel(imds, numTrainFiles,<span class="string">'randomize'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line">data = [];</span><br><span class="line">files = imdsTrain.Files;</span><br><span class="line">x = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> file = files'</span><br><span class="line">    im = imread(file{<span class="number">1</span>});</span><br><span class="line">    im = double(im);</span><br><span class="line">    im = im/<span class="number">255</span>;</span><br><span class="line">    data = [data im(:)];</span><br><span class="line"><span class="keyword">end</span> </span><br><span class="line"><span class="comment">%%</span></span><br><span class="line">X_train = data';</span><br><span class="line">y_train = imdsTrain.Labels;</span><br><span class="line">y_train = label_preprocess(y_train);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%%</span></span><br><span class="line">data = []</span><br><span class="line">files = imdsTest.Files;</span><br><span class="line">x = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> file = files'</span><br><span class="line">    im = imread(file{<span class="number">1</span>});</span><br><span class="line">    im = double(im);</span><br><span class="line">    im = im/<span class="number">255</span>;</span><br><span class="line">    data = [data im(:)];</span><br><span class="line"><span class="keyword">end</span> </span><br><span class="line"><span class="comment">%%</span></span><br><span class="line">X_test = data';</span><br><span class="line">y_test = imdsTest.Labels;</span><br><span class="line">y_test = label_preprocess(y_test);</span><br></pre></td></tr></table></figure>
<p>这里我将那24个 if 打包写成了一个新的函数叫label_preprocess，输入的是标签，输出的是onehot encoding后的结果</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[target]</span> = <span class="title">label_preprocess</span><span class="params">(y_train)</span></span></span><br><span class="line">target = [];</span><br><span class="line"><span class="keyword">for</span> y = y_train'</span><br><span class="line">    <span class="keyword">if</span> y == <span class="string">'A'</span></span><br><span class="line">        target=[target [<span class="number">1</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'B'</span></span><br><span class="line">        target=[target [<span class="number">2</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'C'</span></span><br><span class="line">        target=[target [<span class="number">3</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'D'</span></span><br><span class="line">        target=[target [<span class="number">4</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'E'</span></span><br><span class="line">        target=[target [<span class="number">5</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'F'</span></span><br><span class="line">        target=[target [<span class="number">6</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'G'</span></span><br><span class="line">        target=[target [<span class="number">7</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'H'</span></span><br><span class="line">        target=[target [<span class="number">8</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'J'</span></span><br><span class="line">        target=[target [<span class="number">9</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'K'</span></span><br><span class="line">        target=[target [<span class="number">10</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'L'</span></span><br><span class="line">        target=[target [<span class="number">11</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'M'</span></span><br><span class="line">        target=[target [<span class="number">12</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'N'</span></span><br><span class="line">        target=[target [<span class="number">13</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'P'</span></span><br><span class="line">        target=[target [<span class="number">14</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'Q'</span></span><br><span class="line">        target=[target [<span class="number">15</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'R'</span></span><br><span class="line">        target=[target [<span class="number">16</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'S'</span></span><br><span class="line">        target=[target [<span class="number">17</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'T'</span></span><br><span class="line">        target=[target [<span class="number">18</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'U'</span></span><br><span class="line">        target=[target [<span class="number">19</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'V'</span></span><br><span class="line">        target=[target [<span class="number">20</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'W'</span></span><br><span class="line">        target=[target [<span class="number">21</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'X'</span></span><br><span class="line">        target=[target [<span class="number">22</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'Y'</span></span><br><span class="line">        target=[target [<span class="number">23</span>]];</span><br><span class="line">    <span class="keyword">elseif</span> y==<span class="string">'Z'</span></span><br><span class="line">        target=[target [<span class="number">24</span>]];</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span> </span><br><span class="line"></span><br><span class="line">target =ind2vec(target);</span><br><span class="line">target = target';</span><br></pre></td></tr></table></figure>
<h3 id="方法三：直接读取-g格式（仅限本题使用）"><a href="#方法三：直接读取-g格式（仅限本题使用）" class="headerlink" title="方法三：直接读取.g格式（仅限本题使用）"></a>方法三：直接读取.g格式（仅限本题使用）</h3><p>因为jpeg和jpg都是以g结尾，所以可以本题可以直读取g。代码就略了</p>
]]></content>
      <tags>
        <tag>Matlab</tag>
      </tags>
  </entry>
</search>
